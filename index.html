<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--<base target="_blank">-->
  <title>Diganta Misra</title>
  
  <meta name="author" content="Diganta Misra">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/favicon.png">
  <script src="scramble.js"></script>
  <!-- Place this tag in your head or just before your close body tag. -->
  <!-- <script type="text/javascript"
  src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
  </script> -->
  <script language="JavaScript">
	// write a function to show a table row based on the selected dropdown value from select options
	function showrow(rowId) {
	var row = document.getElementById(rowId);
	row.style.display = "";
	}

	</script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <script src="hidebib.js" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
</head>

<body>
  <table style="width:100%;max-width:920px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tbody><tr><td>
          <p style="text-align:center">
              <img src="images/nav.gif" alt="nav_sign" width="400" height="120">
            <br>
            <br>
            <email>
                <font id="email" style="display:inline;">rltnsiaegiuuae@..istemld.a</font>
                  <script>
                emailScramble = new scrambledString(document.getElementById('email'),
                    'emailScramble', 'rltnsiaegiuuae@..istemld.a',
                    [11, 20, 5, 4, 10, 1, 6, 18, 2, 9, 15, 25, 3, 16, 13, 7, 23, 21, 22, 14, 24, 8, 19, 0, 17, 12]);
              </script> </email>
            
            </p>
		
		<!--<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tr style="padding:0px">
				<td style="padding:0px">

				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tr style="padding:0px">
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="index.html" target="_self">Home</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#news" target="_self">News</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#experience" target="_self">Experience</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#research" target="_self">Research</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#education" target="_self">Education</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#achievements" target="_self">Achievements</a>
					</td>          
					</tr>
				</table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:top">
			  <p>
				  I am a <i style="color:rgb(255, 94, 0);">IMPRS-IS + Amazon Science Hub + ELLIS PhD fellow</i> at the <a href="https://institute-tue.ellis.eu/research-groups/deep-models-and-optimization" target="_blank">Deep Models and Optimization (DMO) Research Group</a> affiliated with <a href="https://is.mpg.de/" target="_blank">Max Planck Institut f√ºr Intelligente Systeme (MPI-IS)</a> and <a href="https://institute-tue.ellis.eu/" target="_blank">ELLIS Institute, T√ºbingen</a> jointly supervised by <a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a> and <a href="https://people.epfl.ch/volkan.cevher?lang=en" target="_blank">Volkan Cevher</a> (<a href="https://www.epfl.ch/labs/lions/" target="_blank">Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland</a>). <img src="https://upload.wikimedia.org/wikipedia/commons/c/cb/Flag_of_Germany_and_Switzerland.png" width="25px">
			  </p>
			  <p>
				  My PhD is generously supported via the:
				  <ul>
					<li><a href="https://ellis.eu/" target="_blank">ELLIS PhD Fellowship</a> program &emsp; <img src="https://ellis.eu/uploads/ckeditor/pictures/9/content_ellis_logo1_320_transparent.png" width="40px"> </li>
					<li>Amazon Science Hub PhD Fellowship <img src="images/amazon.png" width="105px"> </li>
					<li><a href="https://imprs.is.mpg.de/" target="_blank">IMPRS-IS Doctoral fellowship</a> &emsp; <img src="images/mpi.png" alt="nthu" width="120" height="30"></li> 
				  </ul> 
			  </p>
              <p>
                I broadly work on robust and interpretable learning under constraints with focus on but not limited to the following domains:
				<ul>
					<li>HyperNetworks</li>
					<li>Mixture of Experts</li>
					<li>Data Centric ML and Agentic Frameworks</li>
					<li>Diffusion Models and Prompting</li> 
					<li>Optimization and Sequence Modeling</li>
					<li>Code Generation Models</li>
				</ul>

				<h3>Extra-curricular:</h3>
					<p>
						<li>‚õ∞Ô∏è: Summited Mt Jbel Toubkal - 4167M (Morocco). <span style="color:rgb(39, 20, 165)">Everest 2028/29</span>.</li> 
						<li>ü™Ç: Sky-diving (13k).</li>
						<li>‚öΩ: Montr√©al Club Soccer (2023).</li>
						<li>üéπ: Contemporary, Classical, Orchestral (Performances: <a href="https://youtu.be/tvq8E1WJPFE?si=vcXxmp_IQRmyG-Ao" target="_blank">1</a>,<a href="https://youtu.be/nWCTeAjF9ug?si=bA1_EcYUomNPupkM" target="_blank">2</a>).</li>
						<li>üé®: Madhubani, Abstract, Watercolors, Fluid art.</li>
						<li>üé§: Debate (English, Odia Regionals), National MUNs.</li>
					</p>
              </p>

              <p style="padding:4.5%;text-align:center">
                <a href="data/CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=LwiJwNYAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
				<a href="https://github.com/digantamisra98" target="_blank">GitHub</a> &nbsp/&nbsp
				<a href="https://blog.paperspace.com/author/diganta/" target="_blank">Blog</a> &nbsp/&nbsp
				<a href="https://drive.google.com/drive/folders/0BzGvqu5YwdSMUTBlSGhZMkpDWHc?resourcekey=0-XzkFUgqpO_zmor2TsfP7xA&usp=sharing" target="_blank">Certificates</a>
				<br>
				<br>
				<a class="github-button" href="https://github.com/sponsors/digantamisra98" data-color-scheme="no-preference: light; light: light; dark: light;" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @digantamisra98 on GitHub">Sponsor</a>
			  </p>
            </td>
            <td style="padding:0.5%;width:60%;max-width:40%;vertical-align:middle">

			<a href="https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/c5ae195f-e639-4f3e-87e0-6199d10d2fb9/dg65n12-e39839ab-ea1c-4f10-81c0-58a5acdc6a13.png/v1/fill/w_791,h_1010/mike_wazowski_meme_png_by_kylewithem_dg65n12-pre.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9MTYzNCIsInBhdGgiOiJcL2ZcL2M1YWUxOTVmLWU2MzktNGYzZS04N2UwLTYxOTlkMTBkMmZiOVwvZGc2NW4xMi1lMzk4MzlhYi1lYTFjLTRmMTAtODFjMC01OGE1YWNkYzZhMTMucG5nIiwid2lkdGgiOiI8PTEyODAifV1dLCJhdWQiOlsidXJuOnNlcnZpY2U6aW1hZ2Uub3BlcmF0aW9ucyJdfQ.cB5_8iLyaxeAgw_R3f64c1uteab3XMgKaOD6nrNNECs"><img style="width:110%;max-width:110%; vertical-align:middle" alt="profile photo" src="https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/c5ae195f-e639-4f3e-87e0-6199d10d2fb9/dg65n12-e39839ab-ea1c-4f10-81c0-58a5acdc6a13.png/v1/fill/w_791,h_1010/mike_wazowski_meme_png_by_kylewithem_dg65n12-pre.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9MTYzNCIsInBhdGgiOiJcL2ZcL2M1YWUxOTVmLWU2MzktNGYzZS04N2UwLTYxOTlkMTBkMmZiOVwvZGc2NW4xMi1lMzk4MzlhYi1lYTFjLTRmMTAtODFjMC01OGE1YWNkYzZhMTMucG5nIiwid2lkdGgiOiI8PTEyODAifV1dLCJhdWQiOlsidXJuOnNlcnZpY2U6aW1hZ2Uub3BlcmF0aW9ucyJdfQ.cB5_8iLyaxeAgw_R3f64c1uteab3XMgKaOD6nrNNECs" class="hoverZoomLink"></a>
              <!-- <a href="https://i.redd.it/ds3vt4mov4141.jpg"><img style="width:130%;max-width:130%; vertical-align:middle" alt="profile photo" src="https://i.redd.it/ds3vt4mov4141.jpg" class="hoverZoomLink"></a> -->
			  <!-- <img style="width:100%;max-width:100%; vertical-align:bottom" alt="profile photo" src="images/yo.png" class="hoverZoomLink"> -->
			  <!-- <a href="https://i.pinimg.com/originals/6f/30/17/6f3017e2ebcbc0cab9a4483698f318b3.png"><img style="width:130%;max-width:130%; vertical-align:middle" alt="profile photo" src="https://i.pinimg.com/originals/6f/30/17/6f3017e2ebcbc0cab9a4483698f318b3.png" class="hoverZoomLink"></a> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
			<a id="news"><h2>News</h2></a>
			<p>
            <div style="width:100%;overflow-y:scroll; height:230px;">
                <ul id="news">
					<li>June 2025: Gave a talk titled "Programmers or Code Charlatans" at the Friday Talks @ T√ºbingen Series. <a href="https://fridaytalks.github.io/" target="_blank">Talk page</a>.</li>
					<li>May 2025: Our work on <a href="shapley2023">Using Shapley interactions to understand how models use structure</a> is accepted to <a href="https://2025.aclweb.org/" target="_blank">ACL 2025</a>.</li>
					<li>May 2025: Gave a talk titled "Programmers or Code Charlatans" at RIKEN AIP 93rd TrustML Young Scientist Seminar. <a href="https://aip.riken.jp/video/trustml-young-scientist-seminar-93-202509/" target="_blank">Talk page</a>.</li>
					<li>March 2025: Was awarded the prestigious <a href="https://imprs.is.mpg.de/" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS) doctoral fellowship</a>.</li>
					<li>March 2025: Our work on <a href="#hyperalign">Hyper-Align: Efficient Modality Alignment via Hypernetworks</a> is accepted to <a href="https://weight-space-learning.github.io/" target="_blank">ICLR Workshop Weight Space Learning, 2025</a>.</li>
					<li>February 2025: Gave a talk titled " LLMs vs. Torch 1.5: Why Your Code Assistant Can't Keep Up" at Microsoft Research India. <a href="https://www.microsoft.com/en-us/research/video/llms-vs-torch-1-5-why-your-code-assistant-cant-keep-up/" target="_blank">Talk page</a>.</li>
					<li>January 2025: Our work on <a href="#mmteb">MMTEB: Massive Multilingual Text Embedding Benchmark</a> is accepted to <a href="https://iclr.cc/" target="_blank">ICLR 2025</a>.</li>
					<li>January 2025: Our work on <a href="#mit2025">Bridging the Data Provenance Gap Across Text, Speech and Video</a> is accepted to <a href="https://iclr.cc/" target="_blank">ICLR 2025</a>.</li>
					<li>January 2025: Our work on <a href="#mit2025">Bridging the Data Provenance Gap Across Text, Speech and Video</a> is now out on arXiv.</li>
					<li>December 2024: Our work on <a href="#big2022">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a> was selected as one of the finalists of TMLR Outstanding Certification and will be presented at ICLR 2025.</li>
					<li>November 2024: Our work on <a href="#aurora">Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</a> is accepted to <a href="https://coling2025.org/" target="_blank">The 31st International Conference on Computational Linguistics (COLING, 2025)</a> (Industry Track).</li>
					<li>September 2024: Our work on <a href="mit2024">Consent in Crisis: The Rapid Decline of the AI Data Commons</a> is accepted to <a href="https://neurips.cc/" target="_blank">NeurIPS Datasets & Benchmarks Track 2024</a> as a Poster paper.</li>
					<li>September 2024: Our work on <a href="#cep">Slight Corruption in Pre-training Data Makes Better Diffusion Models</a> is accepted to <a href="https://neurips.cc/" target="_blank">NeurIPS 2024</a> as a Spotlight paper.</li>
					<li>September 2024: Started as an <a href="https://institute-tue.ellis.eu/" target="_blank">ELLIS PhD Fellow</a> at the <a href="https://is.mpg.de/" target="_blank">Max Planck Institute for Intelligent Systems (MPI-IS)</a> under the supervision of <a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a>.</li>
					<li>August 2024: Our work on <a href="mit2024">Consent in Crisis: The Rapid Decline of the AI Data Commons</a> is now out on ArXiv.</li>
					<li>June 2024: Our work on <a href="#cep">Slight Corruption in Pre-training Data Makes Better Diffusion Models</a> is out now on arXiv.</li>
					<li>April 2024: Our work on <a href="#vp2023">Uncovering the Hidden Cost of Model Compression</a> is accepted at the <a href="https://prompting-in-vision.github.io/index_cvpr24.html" target="_blank">Prompting in Vision workshop, CVPR, 2024</a>.</li>
					<li>April 2024: Our work on <a href="#mamba2024">On the low-shot transferability of [V]-Mamba</a> is accepted at the <a href="https://prompting-in-vision.github.io/index_cvpr24.html" target="_blank">Prompting in Vision workshop, CVPR, 2024</a>.</li>
					<li>April 2024: Our work on <a href="#aurora">Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</a> is now out on arXiv.</li>
					<li>March 2024: Our work on <a href="#shapley2023">Shapley Interactions for Complex Feature Attribution</a> is out now on arXiv.</li>
					<li>March 2024: Our new preprint <a href="#gnocl">Just Say the Name: Online Continual Learning with Category Names Only via Data Generation</a> is now out on arXiv.</li>
					<li>March 2024: Our new preprint <a href="#mamba2024">On the low-shot transferability of [V]-Mamba</a> is now out on arXiv.</li>
					<li>March 2024: I will be starting as an <a href="https://institute-tue.ellis.eu/" target="_blank">ELLIS PhD Fellow</a> at the <a href="https://is.mpg.de/" target="_blank">Max Planck Institute for Intelligent Systems (MPI-IS)</a> under the supervision of <a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a>.</li>
					<li>March 2024: Our work on <a href="#d2sparse2023">$\mathcal{D}^2$-Sparse: Navigating the low data learning regime with sparse networks</a> is accepted (Oral) at the <a href="https://pml4dc.github.io/iclr2024/" target="_blank">5th PML4LRS workshop,ICLR,</a> 2024.</li>
					<li>March 2024: Our work on <a href="#cl4code2024">GitChameleon: Breaking the version barrier for code generation models</a> is accepted to <a href="https://dmlr.ai/" target="_blank">4th DMLR workshop,ICLR</a>, 2024.</li>
					<li>October 2023: Our work on <a href="#moegflow2023">Mitigating Mode Collapse in Sparse Mixture of Experts</a> is accepted to <a href="https://newinml.github.io/" target="_blank">New in ML workshop, NeurIPS</a>, 2023.</li>
					<li>October 2023: Our work on <a href="#shapley2023">Shapley Interactions for Complex Feature Attribution</a> is accepted to <a href="https://attrib-workshop.cc/" target="_blank">NeurIPS ATTRIB workshop</a>, 2023.</li>
					<li>August 2023: Our new preprint on <a href="#vp2023">Reprogramming under constraints</a> is now out on ArXiv.</li>
					<li>August 2023: Gave an invited talk at <a href="https://www.tue.nl/en/research/researchers/mykola-pechenizkiy" target="_blank">TU Eindoven</a> on <a href="https://drive.google.com/file/d/1Ncr2e5dMEbAp2irMOs6m2-PLnOAhJwdl/view?usp=share_link" target="_blank">Learning under constraints</a>.</li>
					<li>June 2023: I will be joining <a href="http://www.humansensing.cs.cmu.edu/">HSL</a>, CMU in Fall 2023 as a Visiting Researcher.</li>
					<li>May 2023: Our work on <a href="#scole2022">Challenging Common Assumptions about Catastrophic Forgetting</a> got accepted to <a href="https://lifelong-ml.cc/">CoLLAs, 2023</a>.</li>
					<li>May 2023: Gave an invited talk at <a href="https://vita-group.github.io/index.html" target="_blank">VITA, UT-Austin</a> on <a href="https://drive.google.com/file/d/1jvLsCzgiBZodq1a2s9nYZJuGcy23Hd_R/view?usp=share_link" target="_blank">Multi-Domain Expert Layers</a>.</li>
					<li>April 2023: Our work on <a href="#big2022">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a> got accepted to <a href="https://openreview.net/forum?id=uyTL5Bvosj&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)">TMLR</a>.</li>
					<li>March 2023: Our work on <a href="#code2023" target="_blank">Pruning CodeBERT for Improved Code-to-Text Efficiency</a> is accepted to the <a href="https://www.sparseneural.net/" target="_blank">Sparsity in Neural Network (SNN)</a> workshop @ ICLR, 2023.</li>
					<li>November 2022: Gave a talk titled <a href="https://youtu.be/WdzWY9xNLJU" target="_blank">Modality agnostic adaptation in deep learning</a> at the IBM Generalisation talk series.</li>
					<li>November 2022: Our work on <a href="#app2022">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://slowdnn-workshop.github.io/index.html" target="_blank">SlowDNN</a> workshop, 2023</a>.</li>
					<li>November 2022: Our work on <a href="#app2022">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://continual-lifelong-learners.github.io/" target="_blank">Continual Lifelong Learning (CLL)</a> workshop at <a href="https://www.acml-conf.org/2022/" target="_blank">ACML, 2022</a>.</li>
					<li>July 2022: Our work on <a href="#app2022">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://www.sparseneural.net/" target="_blank">Sparsity in Neural Network (SNN)</a> workshop, 2022.</li>
					<li>June 2022: Our work on <a href="#scole2022">Scaling the Number of Tasks in Continual Learning</a> got accepted to the <a href="https://lifelong-ml.cc/" target="_blank">CoLLAs 2022 workshop</a>.</li>
					<li>June 2022: Our work on <a href="#app2022">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://dynn-icml2022.github.io/" target="_blank">Dynamic Neural Network (DyNN) workshop</a> at <a href="https://icml.cc/Conferences/2022/" target="_blank">ICML, 2022</a>.</li>
					<li>May 2022: Awarded the MILA Entrepreneurs Grant worth CAD$5,000.</li>
					<li>May 2022: Awarded the <a href = "https://www.ai-week.ca/?utm_source=google-ads&utm_medium=cpc&utm_campaign=ai-week&utm_term=amii%20ai%20week&utm_campaign=AI-Week+%7C+S+%7C+Brand&utm_source=adwords&utm_medium=ppc&hsa_acc=6591753441&hsa_cam=16953749208&hsa_grp=135907011819&hsa_ad=593686735388&hsa_src=g&hsa_tgt=kwd-1650174358069&hsa_kw=amii%20ai%20week&hsa_mt=p&hsa_net=adwords&hsa_ver=3&gclid=CjwKCAjwve2TBhByEiwAaktM1BjAxiVdVUehV3fuuvfAgtH1vgzVT_jb-fmmTT6sbtfQSoxJ1RTJihoCLykQAvD_BwE" target = "_blank">AI Week 2022</a> Student Travel Bursary worth CAD$1,500.</li>
					<li>April 2022: Awarded the <a href="https://www.unique.quebec/2022-unique-excellence-scholarships">UNIQUE AI Excellence Scholarship</a> worth C$10,000.</li>
					<li>April 2022: The preprint of our paper <a href="#app2022"><textbf>APP: Anytime Progressive Pruning</textbf></a> is out now.</li>
					<li>April 2022: I am starting as a researcher at Morgan Stanley.</li>
					<li>March 2022: Awarded the DIRO x Quebec Ministry of Higher Education international students scholarship worth C$4,000.</li>
					<li>February 2022: I will be serving as a Program Committee member for <a href="https://lifelong-ml.cc/" target="_blank">Conference on Lifelong Learning Agents(CoLLA) 2022</a>.</li>
					<li>January 2022: I am selected to be a part of the MILA Winter 2022 Entrepreneurs Cohort.</li>
					<li>December 2021: I will be serving as a teaching assistant for the <a href="https://www.polymtl.ca/programmes/cours/iatech-probabilistes-et-dapprentissage" target="_blank">INF8225: Probabilistic Learning</a> at Polytechnique University taught by <a href="https://mila.quebec/en/person/pal-christopher/" target="_blank">Christopher J. Pal</a> for the Winter 2022 semester.</li>
					<li>September 2021: I will be serving as a reviewer at <a href="http://wacv2022.thecvf.com/home" target="_blank">WACV 2022</a>.</li>
					<li>August 2021: Our <a href="#Big-Bench">fine grained tense modification task</a> was accepted to <a href="https://github.com/google/BIG-bench" target="_blank">Google's Big Bench</a>.</li>
					<li>July 2021: I am also joining the <a href="https://vita-group.github.io/index.html" target="_blank">VITA, UT-Austin</a> as a Visiting Research Scholar to work on sparsity under the guidance of <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Assistant Professor Zhangyang Wang</a>.</li>
					<li>May 2021: We are organizing the Spring Edition of the <a href="https://wandb.ai/site/reproducibility-challenge" target="_blank">Weights & Biases ML Reproducibility Challenge</a>. Visit our page to learn more.</li>
					<li>May 2021: I will be joining <a href="https://mila.quebec/en/" target="_blank">MILA</a> as a graduate student this fall '21.</li>
					<li>January 2021: Our WACV paper's video is now out on YouTube. Watch it <a href="https://www.youtube.com/watch?v=ZW9_2bNF1zo&ab_channel=ComputerVisionFoundationVideos" target="_blank">here</a>.</li>
					<li>January 2021: I will be speaking at the <a href="https://www.youtube.com/playlist?list=PLD80i8An1OEH3ejAj8R8dy74JeSzY8kGt" target="_blank">W&B Deep Learning Salon</a> on <b>"From Smooth Activations to Robustness to Catastrophic Forgetting"</b>. &emsp; I will be joined by <a href="https://maithraraghu.com/" target="_blank">Maithra Raghu</a> from Google Brain. Watch it <a href="https://www.youtube.com/watch?v=1U-7TWysqIg" target="_blank">here</a>.</li>
					<li>December 2020: I'm starting full time as a Machine Learning Engineer at <a href="https://wandb.ai/site" target="_blank">Weights & Biases</a>.</li>
					<li>October 2020: Our paper <a href="https://openaccess.thecvf.com/content/WACV2021/html/Misra_Rotate_to_Attend_Convolutional_Triplet_Attention_Module_WACV_2021_paper.html" target="_blank">Rotate to Attend: Convolutional Triplet Attention Module</a> is accepted to <a href="http://wacv2021.thecvf.com/home" target="_blank">WACV 2021</a>.</li>
					<li>September 2020: Gave a talk on my paper on <i>Mish</i> at the <b>Robert Bosch Bangalore Research Office</b>.</li>
					<li>August 2020: I completed my Undegraduate degree in Electronics and Electrical Engineering from <a href="https://kiit.ac.in/" target="_blank">Kalinga Institute of Industrial Technology (KIIT)</a>.</li>
					<li>August 2020: Gave a talk on <i>Mish and Non-Linear Dynamics</i> at <a href="https://computervisiontalks.github.io/" target="_blank">Computer Vision Talks</a>. Watch <a href="https://youtu.be/whOdg-yrgdI" target="_blank">here</a>.</li>
					<li>July 2020: My paper <a href="https://www.bmvc2020-conference.com/assets/papers/0928.pdf" target="_blank">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a> is accepted at <a href="https://www.bmvc2020-conference.com/" target="_blank">BMVC 2020</a>.</li>
					<li>July 2020: <b>CROWN: A comparison of morphology for Mish, Swish and ReLU</b> produced in collaboration with <a href="https://ideami.com/ideami/" target="_blank">Javier Ideami</a>. Watch <a href="https://www.youtube.com/watch?v=XRGu23hfzaQ" target="_blank">here</a>.</li>
					<li>May 2020: Participated in an AMA for my paper on <b>Mish</b> at the Weights & Biases reading group.</li>
					<li>April 2020: Presented my views and discussed about Data Science on the <a href="https://anchor.fm/theworldisendingpodcast" target="_blank">The World is Ending Podcast</a>. Listen to the episode <a href="https://anchor.fm/theworldisendingpodcast/episodes/Chatting-with-a-data-Science-team-ft-DeepWrex-Technologies-eco2u6" target="_blank">here</a>.</li>
					<li>February 2020: Talk on <i>Mish and Non-Linear Dynamics</i> at <a href="https://www.sicara.ai/" target="_blank">Sicara</a> is out now. Watch <a href="https://youtu.be/T2CRFROKcLM" target="_blank">here</a>.</li>
					<li>February 2020: Podcast episode on Mish at <a href="">Machine Learning Caf&eacute;</a> is out now. Listen <a href="https://open.spotify.com/episode/4sT9sxjSbAKtvJ6hTFg9zc" target="_blank">here</a>.</li>
					<li>November 2019: Presented a talk on my paper on <i>Mish</i> at the <b>University of Athens</b>.</li>
                </ul>
            </div>
			</p>
          </tr>
        </tbody></table>

		<br>
		<br>
		<br>
        
        <table width="100%" border="0" cellspacing="15" cellpadding="10">
          <a id="experience"><heading>&nbsp;&nbsp;&nbsp;Research Experience</heading></a>

		  <tr>
            <td width="15%" valign="center" align="center">
			<figure class="half" style="display:flex">
				<img style="width:120px" src="images/download.jpeg">
				<img style="width:120px" src="images/aurora.png">
			</figure></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Researcher</strong></span><span style="float:right">April. 2024 - Present</span> <br>
                <em><a href="https://aurora-lm.github.io/" target="_blank">Ontocord AI (Aurora-M team)</a></em>
                <br>
                Supervisor: <a href="https://mrcabbage972.github.io/" target="_blank">Victor May</a>, <a href="https://www.linkedin.com/in/huu-ai-machine-learning/" target="_blank">Huu Nguyen</a>
                <br>
				Research Area: Responsible AI, Large Language Models, EU AI Act
				<br>
              </p>
            </td>
          </tr> 

		  <tr>
			<td width="15%" valign="center" align="center">
			<figure class="half" style="display:flex">
				<img style="width:120px" src="https://www.cmu.edu/brand/brand-guidelines/images/wordmarksquare-red-600x600.png">
				<img style="width:120px" src="https://yt3.googleusercontent.com/ytc/AGIKgqN0LjP63WjgfIBb8_OTa2UasJx-eetXgd0uny7o=s900-c-k-c0x00ffffff-no-rj">
			</figure>
			</td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Research Associate I</strong></span><span style="float:right">Oct 2023 - May 2024</span> <br>
                <em>Carnegie Mellon University (CMU), <a href="http://www.humansensing.cs.cmu.edu/" target="_blank">Human Sensing Lab (HSL)</a></em>
                <br>
                Supervisor: <a href="https://www.cs.cmu.edu/~ftorre/index.html" target="_blank">Prof. Fernando De la Torre</a>
                <br>
				Research Area: Transfer of personalization on continual update of diffusion models.
				<br>
              </p>
            </td>
          </tr> 
		 
		  <tr>
            <td width="15%" valign="center" align="center"><img src="https://yt3.ggpht.com/ytc/AKedOLQj45M52xLSICFBv-A0OBtuTZI5b688ty19ziMe=s900-c-k-c0x00ffffff-no-rj" alt="nthu" width="120" height="120"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Machine Learning Researcher</strong></span><span style="float:right">April. 2022 - Feb. 2023</span> <br>
                <em><a href="https://vita-group.github.io/index.html" target="_blank">Morgan Stanley</a></em>
                <br>
                Supervisor: <a href="https://scholar.google.de/citations?user=cfIrwmAAAAAJ&hl=en" target="_blank">Kashif Rasul</a>
                <br>
				Research Area: Continual Learning, Time Series, Model Reprogramming
				<br>
              </p>
            </td>
          </tr> 
		 
		  <tr>
			<td width="15%" valign="center" align="center">
				<figure class="half" style="display:flex">
					<img src="https://upload.wikimedia.org/wikipedia/en/thumb/e/e1/University_of_Texas_at_Austin_seal.svg/1200px-University_of_Texas_at_Austin_seal.svg.png" style="width:80px"> &emsp;
					<img src="images/vita.png" width="170" height="80">
			</figure></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Remote Visiting Research Scholar</strong></span><span style="float:right">Aug. 2021 - Oct. 2023</span> <br>
                <em><a href="https://vita-group.github.io/index.html" target="_blank">VITA</a>, University of Texas at Austin</em>
                <br>
                Supervisor: <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank"> Dr. Zhangyang Wang</a>
                <br>
				Research Area: Sparsity, Robustness and Knowledge Distillation.
				<br>
              </p>
            </td>
          </tr>

          <tr>
            <td width="15%" valign="center" align="center"><img src="images/hku.jpeg" alt="nthu" width="80" height="100"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong><a href="https://www.lsr.hku.hk/member/diganta-misra/" target="_blank">Research Affiliate</a></strong></span><span style="float:right">Feb. 2020 - Present</span> <br>
                <em><a href="https://www.lsr.hku.hk/" target="_blank">Laboratory of Space Research (LSR)</a>, University of Hong Kong</em>
                <br>
                Supervisor: <a href="https://www.physics.hku.hk/people/academic/5206" target="_blank"> Dr. Quentin A. Parker</a>
                <br>
				Research Area: Computer Vision applications in PNe Exploration.
				<br>
              </p>
            </td>
          </tr>

          <tr>
            <td width="15%" valign="center" align="center"><img src="images/bu.png" alt="nthu" width="90" height="100"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Research Intern</strong></span><span style="float:right">Jun. 2018 - Aug. 2018</span> <br>
                <em><a href="https://www.bennett.edu.in/laboratories/" target="_blank">NVIDIA AI Lab,</a> Bennett University</em>
                <br>
                Supervisors: <a href="https://www.gdeepak.com/" target="_blank">Dr. Deepak Garg</a> and <a href="https://sites.google.com/view/ksuneet/home" target="_blank">Dr. Suneet Gupta</a>
                <br>
				Research Area: Large Scale Visual Recognition.
				<br>
              </p>
            </td>
          </tr>
        </table>


		<table width="100%" border="0" cellspacing="15" cellpadding="10">
		<heading>&nbsp;&nbsp;&nbsp;Industrial and Leadership Experience</heading>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/lskape.png" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong><a href="https://landskape-ai.github.io/member/diganta/" target="_blank">Founder, President and Researcher</a></strong></span><span style="float:right">Sept. 2019 - June. 2024</span> <br>
				<em><a href="https://landskape-ai.github.io/" target="_blank">Landskape AI</a></em>
				<br>
				Mentors: <a href="https://sites.google.com/site/jaegulchoo/" target="_blank"> Assc. Prof. Jaegul Choo</a>, <a href="https://ideami.com/ideami/" target="_blank">Javier Ideami</a> and <a href="https://twitter.com/federicolois" target="_blank">Federico Lois</a>
				<br>
				Research Area: Analytical Deep Learning Theory.
				<br>
			</p>
			</td>
		</tr>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/wandb.jpeg" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Machine Learning Engineer</strong></span><span style="float:right">Dec. 2020 - Oct. 2021</span> <br>
				<em><a href="https://wandb.ai/site" target="_blank">Weights & Biases</a></em>
				<br>
				Team: Frameworks and Integrations.
				<br>
			</p>
			</td>
		</tr>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/paperspace.png" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Technical Content Developer</strong></span><span style="float:right">Jun. 2020 - Jan. 2021</span> <br>
				<em><a href="https://www.paperspace.com/" target="_blank">Paperspace</a></em>
				<br>
				<a href="https://blog.paperspace.com/author/diganta/" target="_blank">Blog</a>
				<br>
				Topic Area: Computer Vision (Attention Mechanisms).
				<br>
			</p>
			</td>
		</tr>
		</table>

		

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a id="research"><heading>Publication</heading></a>
			  <select name="pubtype" onchange="showrow(this.value)" style="float:right; margin-right: 10px;">
				<option value="1">All</option>
				<option value="2">Conference</option>
				<option value="3">Journal</option>
				<option value="4">Workshop</option>
				<option value="5">Preprint</option>
				<option value="6">Under Submission</option>
			  </select>
              <br />*indicates equal contribution 
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		<tr id="therow1 therow2" bgcolor="#ffffd0" style="">
        	<td width="15%" valign="center" align="center"><img src="images/landskape-relu-mish-july-20-Fixed.jpg" alt="nthu" width="290" height="240"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#mish">
                    <papertitle>Mish: A Self Regularized Non-Monotonic Neural Activation Function</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>
                  <br>
                  <p></p>
                  <em>BMVC, 2020</em>
                  <div class="paper" id="mish2020">
                    <a class="tog" href="https://github.com/digantamisra98/Mish" target="_blank">project</a> /
                    <a href="https://www.bmvc2020-conference.com/assets/papers/0928.pdf" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('mish_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('mish_bib')">bibtex</a>
                      <p align="justify">
                        <i id="mish_abs">
                            We propose <b>Mish</b>, a novel self-regularized non-monotonic activation function which can be mathematically defined as: $f(x)=xtanh(softplus(x))$. As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision ($AP^{val}_{50}$) by $2.1\%$ in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by $\approx 1 \%$ while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="mish_bib">
						@article{misra2019mish, <br />
							title={Mish: A self regularized non-monotonic neural activation function}, <br />
							author={Misra, Diganta}, <br />
							journal={arXiv preprint arXiv:1908.08681}, <br />
							volume={4}, <br />
							pages={2}, <br />
							year={2019}, <br />
							publisher={CoRR} <br />
						  }
                            
                        </bibtext>
					<a href="https://youtu.be/whOdg-yrgdI" target="_blank">CV Talk Episode</a> /
					<a href="https://open.spotify.com/episode/4sT9sxjSbAKtvJ6hTFg9zc" target="_blank">ML Cafe Episode</a> /
					<a href="https://youtu.be/T2CRFROKcLM" target="_blank">Sicara Talk</a> /
					<a href="https://www.youtube.com/watch?v=1U-7TWysqIg" target="_blank">W&B Salon Episode</a> /
					<a href="https://www.youtube.com/watch?v=XRGu23hfzaQ" target="_blank">CROWN</a> 
					<br>
					<br>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/mish?style=social">
					&emsp;
					<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/Mish?style=social">
					&emsp;
					<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LwiJwNYAAAAJ&citation_for_view=LwiJwNYAAAAJ:u5HHmVD_uO8C" target="_blank"><img src="https://img.shields.io/badge/Citations-2427-lightgrey.svg?style=social&logo=Google Scholar"></a>
					<br>
					<a href="https://wandb.ai/diganta/Mish?workspace=user-diganta" target="_blank"><img src="https://img.shields.io/badge/Dashboard-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
                    &emsp;
					<a href="https://console.paperspace.com/github/digantamisra98/Mish/blob/master/exps/Mish_test.ipynb" target="_blank">
					<img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"/>
					<br>
					</a>
					</div>              
                    <script language="JavaScript">hideblock('mish_bib');hideblock('mish_abs');</script>
            </td>
            </tr>
			
			<tr class="therow1 therow2" style="">
			<td width="15%" valign="center" align="center"><img src="images/grad.png" alt="nthu" width="220" height="250"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#triplet">
                    <papertitle>Rotate to Attend: Convolutional Triplet Attention Module</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra<sup>*</sup></strong>,
				  <a href="https://scholar.google.com/citations?user=1YmHR6MAAAAJ&hl=en" target="_blank">Trikay Nalamada</a><sup>*</sup>,
				  <a href="https://iyaja.github.io/" target="_blank">Ajay Uppili Arasanipalai</a><sup>*</sup>,
				  <a href="https://andrew-qibin.github.io/homepage/" target="_blank">Qibin Hou</a>
				  <br>
                  <p></p>
                  <em>WACV, 2021</em>
                  <div class="paper" id="wacv2021">
                    <a class="tog" href="https://github.com/landskape-ai/triplet-attention" target="_blank">project</a> /
                    <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Misra_Rotate_to_Attend_Convolutional_Triplet_Attention_Module_WACV_2021_paper.pdf" target="_blank">paper</a> /
					<a href="https://openaccess.thecvf.com/content/WACV2021/supplemental/Misra_Rotate_to_Attend_WACV_2021_supplemental.pdf" target="_blank">supplementary</a> /
					<a href="https://youtu.be/ZW9_2bNF1zo" target="_blank">video</a> /
                      <a class="tog" href="javascript:toggleblock('wacv_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('wacv_bib')">bibtex</a>
                      <p align="justify">
                        <i id="wacv_abs">
                            Benefiting from the capability of building interdependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly
							used in a variety of computer vision tasks recently. In
							this paper, we investigate light-weight but effective attention mechanisms and present <b>triplet attention</b>, a novel
							method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For
							an input tensor, triplet attention builds inter-dimensional
							dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial
							information with negligible computational overhead. Our
							method is simple as well as efficient and can be easily
							plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on
							various challenging tasks including image classification on
							ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive insight into the performance of triplet attention by visually
							inspecting the GradCAM and GradCAM++ results. The
							empirical evaluation of our method supports our intuition
							on the importance of capturing dependencies across dimensions when computing attention weights.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="wacv_bib">
						@inproceedings{misra2021rotate, <br />
						title={Rotate to attend: Convolutional triplet attention module}, <br />
						author={Misra, Diganta and Nalamada, Trikay and Arasanipalai, Ajay Uppili and Hou, Qibin}, <br />
						booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, <br />
						pages={3139--3148}, <br />
						year={2021} <br />
						}
                            
                        </bibtext>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/landskapeai/triplet-attention?style=social">
					&emsp;
					<img alt="GitHub forks" src="https://img.shields.io/github/forks/landskapeai/triplet-attention?style=social">
					&emsp;
					<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LwiJwNYAAAAJ&citation_for_view=LwiJwNYAAAAJ:UeHWp8X0CEIC" target="_blank"><img src="https://img.shields.io/badge/Citations-983-lightgrey.svg?style=social&logo=Google Scholar"></a>
                    </div>              
                    <script language="JavaScript">hideblock('wacv_bib');hideblock('wacv_abs');</script>
            </td>
			</tr>



			<tr class="therow1 therow6" style="">
				<td width="15%" valign="center" align="center"><img src="images/consonant.jpg" alt="nthu" width="500" height="120"></td>
				</td>
				<td width="85%" style="padding:20px;vertical-align:middle">
					  <a class="tog" href="#shapley2023">
						<papertitle>Using Shapley interactions to understand how models use structure &emsp14; <span style="color:red">New!</span></papertitle>
					  </a>
					  <br>
					  <a href="https://divyanshsinghvi.github.io/" target="_blank">Divyansh Singhvi</a><sup>*</sup>, 
					  <strong>Diganta Misra</strong><sup>*</sup>,
					  <a href="https://www.linkedin.com/in/andrej-erkelens" target="_blank">Andrej Erkelens</a><sup>*</sup>, 
					  <a href="https://www.linkedin.com/in/raghav-jain-3a8076214/" target="_blank">Raghav Jain</a><sup>*</sup>,
					  <a href="https://www.isabelpapad.com/" target="_blank">Isabel Papadimitriou</a>,
					  <a href="https://nsaphra.net/" target="_blank">Naomi Saphra</a>
					  <br>
					  <p></p>
					  <em>ACL, 2025</em>
					  <br>
					  <em>NeurIPS ATTRIB workshop, 2023</em>
					  <br>
					  <div class="paper" id="shapley2023">
						<a href="https://arxiv.org/abs/2403.13106" target="_blank">paper</a> /
						  <a class="tog" href="javascript:toggleblock('shapley_abs')">abstract</a> /
						  <a class="tog" href="javascript:toggleblock('shapley_bib')">bibtex</a>
						  <br>
						  <br>
						  <a href="https://nitter.net/nsaphra/status/1933202363495370969#m" target="_blank"><img alt="Twitter thread" src="https://img.shields.io/badge/Thread-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white" width="70"></a>
						  <p align="justify">
							<i id="shapley_abs">
								Language is an intricately structured system, and a key goal of NLP interpretability is to provide methodological insights for understanding how language models internally represent this structure. In this paper, we use Shapley Taylor interaction indices (STII) in order to examine how language and speech models internally relate and structure their inputs. Pairwise Shapley interactions give us an attribution measure of how much two inputs work together to influence model outputs beyond if we linearly added their independent influences, providing a view into how models encode structural interactions between inputs. We relate the interaction patterns in models to three underlying linguistic structures: syntactic structure, non-compositional semantics, and phonetic interaction. We find that autoregressive text models encode interactions that correlate with the syntactic proximity of inputs, and that both autoregressive and masked models encode nonlinear interactions in idiomatic phrases with non-compositional semantics. Our speech results show that inputs are more entangled for pairs where a neighboring consonant is likely to influence a vowel or approximant, showing that models encode the phonetic interaction needed for extracting discrete phonemic representations.</i>
							<bibtext xml:space="preserve" id="shapley_bib">
								@misc{singhvi2024knowing,
									title={Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data}, 
									author={Divyansh Singhvi and Andrej Erkelens and Raghav Jain and Diganta Misra and Naomi Saphra},
									year={2024},
									eprint={2403.13106},
									archivePrefix={arXiv},
									primaryClass={cs.LG}
							  }
							</bibtext>
						  </p>
						</div>              
						<script language="JavaScript">hideblock('shapley_abs');hideblock('shapley_bib');</script>
				</td>
				</tr>

				<tr class="therow1 therow6" style="">
				<td width="15%" valign="center" align="center"><img src="images/IMG_0055-removebg-preview.png" alt="nthu" width="200" height="200"></td>
				</td>
				<td width="85%" style="padding:20px;vertical-align:middle">
					  <a class="tog" href="#cl4code2024">
						<papertitle><span style="color:rgb(136, 1, 247)">GitChameleon</span>: Evaluating AI Code Generation Against Python Library Version Incompatibilities &emsp14; <span style="color:red">New!</span></papertitle>
					  </a>
					  <br>
					  	<strong>Diganta Misra</strong><sup>*</sup>,
						<a href="https://nizarislah.github.io/" target="_blank">Nizar Islah</a><sup>*</sup>,
						<a href="https://mrcabbage972.github.io/" target="_blank">Victor May</a>,
						<a href="https://scholar.google.com/citations?user=b7KZuocAAAAJ&hl=en" target="_blank">Brice Rauby</a>,
						<a href="https://mila.quebec/en/directory/zihan-wang" target="_blank">Zihan Wang</a>,
					  	<a href="https://justine-gehring.github.io/" target="_blank">Justine Gehring</a>,
						<a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a>,
						<a href="https://scholar.google.com/citations?user=4Z8ePskAAAAJ&hl=en" target="_blank">Muawiz Chaudhary</a>,
						<a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a>,
						<a href="https://neuro.plasticit.ai/" target="_blank">Eilif B. Muller</a>,
						<a href="https://saebrahimi.github.io/" target="_blank">Samira Ebrahimi Kahou</a>,
						<a href="https://optimass.github.io/" target="_blank">Massimo Caccia</a>
					  <br>
					  <p></p>
					  <em>DMLR @ ICLR, 2024</em>
					  <br>
					  <div class="paper" id="cl4code">
						<a href="https://arxiv.org/abs/2411.05830" target="_blank">paper</a> /
						  <a class="tog" href="javascript:toggleblock('cl4code_abs')">abstract</a> /
						  <a class="tog" href="javascript:toggleblock('cl4code_bib')">bibtex</a>
						  <p align="justify">
							<i id="cl4code_abs">
								The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to generate version-specific code that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We will publicly release the dataset and evaluation code upon acceptance.
							</i>
						  </p>
						<bibtext xml:space="preserve" id="cl4code_bib">
							@misc{islah2024gitchameleonunmaskingversionswitchingcapabilities,
								title={GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models}, 
								author={Nizar Islah and Justine Gehring and Diganta Misra and Eilif Muller and Irina Rish and Terry Yue Zhuo and Massimo Caccia},
								year={2024},
								eprint={2411.05830},
								archivePrefix={arXiv},
								primaryClass={cs.SE},
								url={https://arxiv.org/abs/2411.05830}, 
						  }
						</bibtext>
						<a href="https://youtu.be/8s1hX6Xw_3Q?si=KlrLv1nIqex2qOHh" target="_blank"><img src="https://img.shields.io/badge/Microsoft Research Talk-red?style=for-the-badge&logo=youtube&logoColor=white"></a>
						<a href="https://www.youtube.com/watch?v=Ccg4RNTYkVs" target="_blank"><img src="https://img.shields.io/badge/RIKEN AIP 93rd TrustML Young Scientist Seminar-red?style=for-the-badge&logo=youtube&logoColor=white"></a>
						</div>  
								<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NizarIslah/GitChameleon?style=social">
								&emsp;
								<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6694357941698904074,11417814403370446599" target="_blank"><img src="https://img.shields.io/badge/Citations-1-lightgrey.svg?style=social&logo=Google Scholar"></a>
						<script language="JavaScript">hideblock('cl4code_abs');;hideblock('cl4code_bib');</script>
				</td>
				</tr>

				<tr class="therow1 therow6" style="">
					<td width="15%" valign="center" align="center"><img src="images/Cep.png" alt="nthu" width="380" height="250"></td>
					</td>
					<td width="85%" style="padding:20px;vertical-align:middle">
						  <a class="tog" href="#cep">
							<papertitle>Slight Corruption in Pre-training Data Makes Better Diffusion Models</papertitle>
						  </a>
						  <br>
						  <br>
							<a href="https://scholar.google.com/citations?user=tktqkhwAAAAJ&hl=en" target="_blank">Hao Chen</a>,
							<a href="https://yujinhanml.github.io/" target="_blank">Yujin Han</a>,
							<strong>Diganta Misra</strong>,
							<a href="https://lxa9867.github.io/" target="_blank">Xiang Li</a>,
							<a href="https://scholar.google.com/citations?user=Dn1rSvkAAAAJ&hl=en" target="_blank">Kai Hu</a>,
							<a href="https://difanzou.github.io/" target="_blank">Difan Zou</a>,
							<a href="https://www.ms.k.u-tokyo.ac.jp/sugi/index.html" target="_blank">Masashi Sugiyama</a>,
							<a href="https://jd92.wang/" target="_blank">Jindong Wang</a>,
							<a href="http://mlsp.cs.cmu.edu/people/bhiksha/" target="_blank">Bhiksha Raj</a>
						  <br>
						  <p></p>
						  <em>NeurIPS <span style="color:rgb(255, 0, 166)">Spotlight</span>, 2024</em>
						  <br>
						  <div class="paper" id="cep">
							<a href="https://arxiv.org/abs/2403.10853" target="_blank">paper</a> /
							  <a class="tog" href="javascript:toggleblock('cep_abs')">abstract / 
							  <a class="tog" href="javascript:toggleblock('cep_bib')">bibtex</a>
							  <br>
							  <br>
						  		<a href="https://x.com/__z__9/status/1797770731499225583" target="_blank"><img alt="Twitter" src="https://img.shields.io/badge/Thread-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white" width="70"></a>
								  <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9784087597028682083" target="_blank"><img src="https://img.shields.io/badge/Citations-7-lightgrey.svg?style=social&logo=Google Scholar"></a>
							  <p align="justify">
								<i id="cep_abs">
									Diffusion models (DMs) have shown remarkable capabilities in generating realistic high-quality images, audios, and videos. They benefit significantly from extensive pre-training on large-scale datasets, including web-crawled data with paired data and conditions, such as image-text and image-class pairs. Despite rigorous filtering, these pre-training datasets often inevitably contain corrupted pairs where conditions do not accurately describe the data. This paper presents the first comprehensive study on the impact of such corruption in pre-training data of DMs. We synthetically corrupt ImageNet-1K and CC3M to pre-train and evaluate over 50 conditional DMs. Our empirical findings reveal that various types of slight corruption in pre-training can significantly enhance the quality, diversity, and fidelity of the generated images across different DMs, both during pre-training and downstream adaptation stages. Theoretically, we consider a Gaussian mixture model and prove that slight corruption in the condition leads to higher entropy and a reduced 2-Wasserstein distance to the ground truth of the data distribution generated by the corruptly trained DMs. Inspired by our analysis, we propose a simple method to improve the training of DMs on practical datasets by adding condition embedding perturbations (CEP). CEP significantly improves the performance of various DMs in both pre-training and downstream tasks. We hope that our study provides new insights into understanding the data and pre-training processes of DMs.
								</i>
							  </p>
							 
							<bibtext xml:space="preserve" id="cep_bib">
								@misc{chen2024slight,
									title={Slight Corruption in Pre-training Data Makes Better Diffusion Models}, 
									author={Hao Chen and Yujin Han and Diganta Misra and Xiang Li and Kai Hu and Difan Zou and Masashi Sugiyama and Jindong Wang and Bhiksha Raj},
									year={2024},
									eprint={2405.20494},
									archivePrefix={arXiv},
									primaryClass={cs.CV}
							  }	
								</bibtext> 
							<br>
							</div>        
							<script language="JavaScript">hideblock('cep_abs');hideblock('cep_bib');</script>
					</td>
					</tr>
			

			<tr class="therow1 therow6" style="">
				<td width="15%" valign="center" align="center"><img src="images/gencl.png" alt="nthu" width="380" height="110"></td>
				</td>
				<td width="85%" style="padding:20px;vertical-align:middle">
					  <a class="tog" href="#gnocl">
						<papertitle>Just Say the Name: Online Continual Learning with Category Names Only via Data Generation</papertitle>
					  </a>
					  <br>
					  <br>
					  	<a href="https://scholar.google.com/citations?user=ayDPR-gAAAAJ&hl=ko" target="_blank">Minhyuk Seo</a>,
						Seongwon Cho,
						<a href="https://kr.linkedin.com/in/minjae-lee-307102210" target="_blank">MinJae Lee</a>,
						<strong>Diganta Misra</strong>,
						Hyeonbeom Choi,
						<a href="https://sites.google.com/site/seonjookim/" target="_blank">Seon Joo Kim</a>,
						<a href="https://ppolon.github.io/" target="_blank">Jonghyun Choi Ïµú</a>
					  <br>
					  <p></p>
					  <em>Preprint</em>
					  <br>
					  <div class="paper" id="gnocl">
						<a href="https://arxiv.org/abs/2403.10853" target="_blank">paper</a> /
						  <a class="tog" href="javascript:toggleblock('gnocl_abs')">abstract / 
						  <a class="tog" href="javascript:toggleblock('gnocl_bib')">bibtex</a>
						  <br>
						  <br>
						  <a href="https://x.com/__z__9/status/1769911791117578518?s=20" target="_blank"><img alt="Twitter" src="https://img.shields.io/badge/Thread-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white" width="70"></a>
						  <a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2240813127000309223" target="_blank"><img src="https://img.shields.io/badge/Citations-9-lightgrey.svg?style=social&logo=Google Scholar"></a>
						  <p align="justify">
							<i id="gnocl_abs">
								In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distribution (ID) and Out-of-Distribution (OOD) generalization evaluations, compared to naive generator-ensembling, web-supervised, and manually annotated data.
							</i>
						  </p>
						 
						<bibtext xml:space="preserve" id="gnocl_bib">
							@misc{seo2024just,
								title={Just Say the Name: Online Continual Learning with Category Names Only via Data Generation}, 
								author={Minhyuk Seo and Diganta Misra and Seongwon Cho and Minjae Lee and Jonghyun Choi},
								year={2024},
								eprint={2403.10853},
								archivePrefix={arXiv},
								primaryClass={cs.LG}
						  }
								
							</bibtext> 
						<a href="https://us06web.zoom.us/rec/share/XFGNojMl5oE4t2VS05GL7HwJzuI3vKNPHfYBTFAOATtKS_F5w_Y9fQ1-ru4GMS6T.sHLkTu0_o6s9NPEJ?startTime=1711729152000&pwd=DVzpvgxgmY_fmHcXlajAYPXu0HaR_Yyc" target="_blank">Mila CERC AAI AI & Scale Workshop 2024 Talk</a>
						<br>
						<br>
						<a href="https://youtu.be/FJpRORxIbC4" target="_blank"><img src="https://img.shields.io/badge/Efficient Machine Learning: Reading Group-red?style=for-the-badge&logo=youtube&logoColor=white"></a>
						<br>
						<a href="https://youtu.be/Tl85REdx_3s?si=6Caz3XohjlKQlJFY" target="_blank"><img src="https://img.shields.io/badge/DLCT-red?style=for-the-badge&logo=youtube&logoColor=white"></a>
						</div>        
						<script language="JavaScript">hideblock('gnocl_abs');hideblock('gnocl_bib');</script>
				</td>
				</tr>


					<tr class="therow1 therow6" style="">
						<td width="15%" valign="center" align="center"><img src="images/hyma.jpg" alt="nthu" width="500" height="200"></td>
						</td>
						<td width="85%" style="padding:20px;vertical-align:middle">
							  <a class="tog" href="#hyperalign">
								<papertitle>(Almost) Free Modality Alignment of Foundation Models &emsp14; <span style="color:red">New!</span></papertitle>
							  </a>
							  <br>
							  <a href="https://jaisidhsingh.github.io/" target="_blank">Jaisidh Singh</a>, 
							  <strong>Diganta Misra</strong>,
							  <a href="https://bknyaz.github.io/" target="_blank">Boris Knyazev</a>, 
							  <a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a>
							  <br>
							  <p></p>
							  <em>ICLR Workshop on Workshop on Weight Space Learning, 2025</em>
							  <br>
							  <div class="paper" id="hyperalign">
								  <a class="tog" href="javascript:toggleblock('hyperalign_abs')">abstract</a> /
								  <a class="tog" href="javascript:toggleblock('hyperalign_bib')">bibtex</a> 
								  <p align="justify">
									<i id="hyperalign_abs">
										Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with an autoregressive text model. This stitching process is performed by training a connector module that aims to align the representation-representation or representation-input spaces of these uni-modal models. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hyma, a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for N x M combinations of uni-modal models. In our experiments, Hyma reduces the optimal uni-modal model pair search cost by 11x on average, while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks.
									</i>
								  </p>
								  <bibtext xml:space="preserve" id="hyperalign_bib">
									@inproceedings{singhhyper,
										title={Hyper-Align: Efficient Modality Alignment via Hypernetworks},
										author={Singh, Jaisidh and Misra, Diganta and Knyazev, Boris and Orvieto, Antonio},
										booktitle={Workshop on Neural Network Weights as a New Data Modality}
									  }
										
									</bibtext>  
								</div>              
								<script language="JavaScript">hideblock('hyperalign_abs');hideblock('hyperalign_bib');</script>
						</td>
						</tr>

				<tr class="therow1 therow3" style="">
					<td width="15%" valign="center" align="center"><img src="https://avatars.githubusercontent.com/u/145158135?s=280&v=4" alt="nthu" width="180" height="180"></td>
					</td>
					<td width="85%" style="padding:20px;vertical-align:middle">
						  <a class="tog" href="#mit2024">
							<papertitle>Consent in Crisis: The Rapid Decline of the AI Data Commons</papertitle>
						  </a>
						  <br>
						  <strong>Diganta Misra</strong>,
						  Multiple authors
						  <br>
						  <p></p>
						  <em>NeurIPS D&B track, 2024</em>
						  <div class="paper" id="mit2024">
							<a class="tog" href="https://www.dataprovenance.org/" target="_blank">Data Provenence Institute</a> /
							<a href="https://arxiv.org/abs/2407.14933" target="_blank">paper</a> /
							  <a class="tog" href="javascript:toggleblock('mit_abs')">abstract</a> / 
							  <a class="tog" href="javascript:toggleblock('mit_bib')">bibtex</a>
							  <p align="justify">
								<i id="mit_abs">
									General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. Our audit of 14,000 web domains provides an expansive view of crawlable web data and how codified data use preferences are changing over time. We observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. We diagnose these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. Our longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. We hope to illustrate the emerging crises in data consent, for both developers and creators. The foreclosure of much of the open web will impact not only commercial AI, but also non-commercial AI and academic research.
								</i>
							  </p>
							  <bibtext xml:space="preserve" id="mit_bib">
								@misc{longpre2024consentcrisisrapiddecline,
									title={Consent in Crisis: The Rapid Decline of the AI Data Commons}, 
									author={Shayne Longpre and Robert Mahari and Ariel Lee and Campbell Lund and Hamidah Oderinwale and William Brannon and Nayan Saxena and Naana Obeng-Marnu and Tobin South and Cole Hunter and Kevin Klyman and Christopher Klamm and Hailey Schoelkopf and Nikhil Singh and Manuel Cherep and Ahmad Anis and An Dinh and Caroline Chitongo and Da Yin and Damien Sileo and Deividas Mataciunas and Diganta Misra and Emad Alghamdi and Enrico Shippole and Jianguo Zhang and Joanna Materzynska and Kun Qian and Kush Tiwary and Lester Miranda and Manan Dey and Minnie Liang and Mohammed Hamdy and Niklas Muennighoff and Seonghyeon Ye and Seungone Kim and Shrestha Mohanty and Vipul Gupta and Vivek Sharma and Vu Minh Chien and Xuhui Zhou and Yizhi Li and Caiming Xiong and Luis Villa and Stella Biderman and Hanlin Li and Daphne Ippolito and Sara Hooker and Jad Kabbara and Sandy Pentland},
									year={2024},
									eprint={2407.14933},
									archivePrefix={arXiv},
									primaryClass={cs.CL},
									url={https://arxiv.org/abs/2407.14933}, 
							  	}	
								</bibtext>
		
							<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=593019343386393612" target="_blank"><img src="https://img.shields.io/badge/Citations-46-lightgrey.svg?style=social&logo=Google Scholar"></a>
							<br>
							<br>
							<span style="color:rgb(200, 0, 255)">Media Coverage</span>:
							<br>
							<br>
							<a href="https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html" target="_blank"><img src="images/nyt.jpg" width="30px"></a>
							&emsp13;
							<a href="https://observer.com/2024/07/ai-training-data-crisis/" target="_blank"><img src="https://pbs.twimg.com/profile_images/1017384553646841859/TgR1X0KS_400x400.jpg" width="40px"></a>
							&emsp13;
							<a href="https://x.com/YahooFinance/status/1823838256128995684" target="_blank"><img src="https://s.yimg.com/uu/api/res/1.2/l_wEBH56oWboJLadyoDWTg--~B/aD02NDg7dz02NDg7YXBwaWQ9eXRhY2h5b24-/https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2019-09/1f16fbf0-de0a-11e9-9af9-77bfe97840a9" width="40px"></a>
							&emsp13;
							<a href="https://x.com/mozilla/status/1816193134788825342" target="_blank"><img src="https://cdn.worldvectorlogo.com/logos/mozilla-foundation.svg" width="45px"></a>
							&emsp13;
							<a href="https://www.404media.co/websites-are-blocking-the-wrong-ai-scrapers-because-ai-companies-keep-making-new-ones/" target="_blank"><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS60fft9itll9BA0wQICLqOrlNo4fLuNcYFfA&s" width="95px"></a>
							<br>
							<br>
							<a href="https://futurism.com/the-byte/ai-companies-losing-training-data" target="_blank"><img src="https://futurism.com/schema/WebsiteFuturism.jpg" width="45px"></a>
							&emsp13;
							<a href="https://flowingdata.com/2024/08/01/decline-in-data-for-ai-bots-to-scrape/" target="_blank"><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTsLveFv6AzrLJPzscA7fi7M9AIAQU8ij53Kg&s" width="45px"></a>
							&emsp13;
							<a href="https://www.datanami.com/2024/07/26/are-we-running-out-of-training-data-for-genai/" target="_blank"><img src="https://voxuspr.com/wp-content/uploads/2024/01/6578e93a1688079101428c12_ai-media-logo-9-datanami.png" width="85px"></a>
							&emsp13;
							<a href="https://www.fortuneidn.com/tech/bayu/studi-mit-sebut-data-untuk-latih-teknologi-ai-akan-habis?page=all" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/3/3c/Fortune_Indonesia_Logo.webp" width="85px"></a>
							</div>             
							<script language="JavaScript">hideblock('mit_bib');hideblock('mit_abs');</script>
					</td>
					</tr>


					<tr class="therow1 therow3" style="">
						<td width="15%" valign="center" align="center"><img src="https://avatars.githubusercontent.com/u/145158135?s=280&v=4" alt="nthu" width="180" height="180"></td>
						</td>
						<td width="85%" style="padding:20px;vertical-align:middle">
							  <a class="tog" href="#mit2025">
								<papertitle>Bridging the Data Provenance Gap Across Text, Speech and Video</papertitle>
							  </a>
							  <br>
							  <strong>Diganta Misra</strong>,
							  Multiple authors
							  <br>
							  <p></p>
							  <em>ICLR 2025</em>
							  <div class="paper" id="mit2025">
								<a class="tog" href="https://www.dataprovenance.org/" target="_blank">Data Provenence Institute</a> /
								<a href="https://arxiv.org/abs/2412.17847" target="_blank">paper</a> /
								  <a class="tog" href="javascript:toggleblock('mit25_abs')">abstract</a> / 
								  <a class="tog" href="javascript:toggleblock('mit25_bib')">bibtex</a>
								  <p align="justify">
									<i id="mit25_abs">
										Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.
									</i>
								  </p>
								  <bibtext xml:space="preserve" id="mit25_bib">
									@misc{longpre2024bridgingdataprovenancegap,
										title={Bridging the Data Provenance Gap Across Text, Speech and Video}, 
										author={Shayne Longpre and Nikhil Singh and Manuel Cherep and Kushagra Tiwary and Joanna Materzynska and William Brannon and Robert Mahari and Manan Dey and Mohammed Hamdy and Nayan Saxena and Ahmad Mustafa Anis and Emad A. Alghamdi and Vu Minh Chien and Naana Obeng-Marnu and Da Yin and Kun Qian and Yizhi Li and Minnie Liang and An Dinh and Shrestha Mohanty and Deividas Mataciunas and Tobin South and Jianguo Zhang and Ariel N. Lee and Campbell S. Lund and Christopher Klamm and Damien Sileo and Diganta Misra and Enrico Shippole and Kevin Klyman and Lester JV Miranda and Niklas Muennighoff and Seonghyeon Ye and Seungone Kim and Vipul Gupta and Vivek Sharma and Xuhui Zhou and Caiming Xiong and Luis Villa and Stella Biderman and Alex Pentland and Sara Hooker and Jad Kabbara},
										year={2024},
										eprint={2412.17847},
										archivePrefix={arXiv},
										primaryClass={cs.AI},
										url={https://arxiv.org/abs/2412.17847}, 
								  }
									</bibtext>

								<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17924853915423055051" target="_blank"><img src="https://img.shields.io/badge/Citations-2-lightgrey.svg?style=social&logo=Google Scholar"></a>
								<br>
								<br>
	
								<span style="color:rgb(200, 0, 255)">Media Coverage</span>:
								<br>
								<br>
								<a href="https://www.technologyreview.com/2024/12/18/1108796/this-is-where-the-data-to-build-ai-comes-from/" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/8/86/MIT_Technology_Review_modern_logo.svg" width="70px"></a>
								</div>             
								<script language="JavaScript">hideblock('mit25_bib');hideblock('mit25_abs');</script>
						</td>
						</tr>

						<tr class="therow1 therow3" style="">
							<td width="15%" valign="center" align="center"><img src="images/mmteb_overview_wide_centered.png" alt="nthu" width="240" height="120"></td>
							</td>
							<td width="85%" style="padding:20px;vertical-align:middle">
								  <a class="tog" href="#mmteb">
									<papertitle>MMTEB: Massive Multilingual Text Embedding Benchmark</papertitle>
								  </a>
								  <br>
								  <strong>Diganta Misra</strong>,
								  Multiple authors
								  <br>
								  <p></p>
								  <em>ICLR 2025</em>
								  <div class="paper" id="mmteb">
									<a href="https://openreview.net/forum?id=zl3pfz4VCV" target="_blank">paper</a> /
									  <a class="tog" href="javascript:toggleblock('mmteb_abs')">abstract</a> /
									  <a class="tog" href="javascript:toggleblock('mmteb_bib')">bibtex</a>
									  <p align="justify">
										<i id="mmteb_abs">
											Text embeddings are typically evaluated on a narrow set of tasks, limited in terms of languages, domains, and task types. To circumvent this limitation and to provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) -- a large-scale community-driven initiative expanding MTEB to over 500 \textit{quality controlled} evaluation tasks across 1,000+ languages. MMTEB includes a wide range of challenging novel tasks such as instruction following, long-document retrieval, and code retrieval, and represents the largest multilingual collection of evaluation tasks for embedding models to date. We use this collection to construct multiple highly multilingual benchmarks. We evaluate a representative set of models on these benchmarks. Our findings indicate that, while LLM-based models can achieve state-of-the-art performance on a subset of languages, the best-performing publicly available model across languages is the notably smaller, multilingual-e5-large-instruct. Massive benchmarks often impose high computational demands, limiting accessibility, particularly for low-resource communities. To address this, we downsample tasks based on inter-task correlation (i.e., selecting only a diverse set of tasks) while preserving relative rankings. We further optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks at a significantly lower computational cost. For instance, we introduce a new zero-shot English benchmark that maintains a similar ordering at a fraction of the cost.
										</i>
									  </p>
									  <bibtext xml:space="preserve" id="mmteb_bib">
										@misc{enevoldsen2025mmtebmassivemultilingualtext,
											title={MMTEB: Massive Multilingual Text Embedding Benchmark}, 
											author={Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M√°rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi≈Ñski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr√∏m and Roman Solomatin and √ñmer √áaƒüatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa≈Ç Po≈õwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj√∂rn Pl√ºster and Jan Philipp Harries and Lo√Øc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek ≈†uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G√ºnther and Mengzhou Xia and Weijia Shi and Xing Han L√π and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},
											year={2025},
											eprint={2502.13595},
											archivePrefix={arXiv},
											primaryClass={cs.CL},
											url={https://arxiv.org/abs/2502.13595}, 
									  }
										</bibtext>
				
									<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4163901252801811115" target="_blank"><img src="https://img.shields.io/badge/Citations-8-lightgrey.svg?style=social&logo=Google Scholar"></a>
									  <br>
									</div>             
									<script language="JavaScript">hideblock('mmteb_abs');hideblock('mmteb_bib');</script>
							</td>
							</tr>
	
				<tr class="therow1 therow3" style="">
				<td width="15%" valign="center" align="center"><img src="images/bb.png" alt="nthu" width="230" height="230"></td>
				</td>
				<td width="85%" style="padding:20px;vertical-align:middle">
					  <a class="tog" href="#big2022">
						<papertitle>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</papertitle>
					  </a>
					  <br>
					  <strong>Diganta Misra</strong>,
					  <a href="https://in.linkedin.com/in/mukundvarmat" target="_blank">Mukund Varma T.</a>,
					  Multiple authors
					  <br>
					  <p></p>
					  <em>TMLR, 2023</em>
					  <br>
					  <em>TMLR Finalist for Outstanding Certification</em>
					  <br>
					  <em>ICLR 2025</em>
					  <div class="paper" id="big2022">
						<a class="tog" href="https://github.com/google/BIG-bench" target="_blank">project</a> /
						<a href="https://arxiv.org/abs/2206.04615" target="_blank">paper</a> /
						  <a class="tog" href="javascript:toggleblock('big_abs')">abstract</a> / 
						  <a class="tog" href="javascript:toggleblock('big_bib')">bibtex</a>
						  <p align="justify">
							<i id="big_abs">
								Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.
							</i>
						  </p>
						  <bibtext xml:space="preserve" id="big_bib">
							@article{srivastava2022beyond,<br />
								title   = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},<br />
								author  = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri√† Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Santilli and Andreas Stuhlm√ºller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka≈ü and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart≈Çomiej Bojanowski and Batuhan √ñzyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C√©sar Ferri Ram√≠rez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Mosegu√≠ Gonz√°lez and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart√≠nez-Plumed and Francesca Happ√© and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ√°n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-L√≥pez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Sch√ºtze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern√°ndez Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco≈Ñ and Jana Thompson and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Berant and J√∂rg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col√≥n and Luke Metz and L√ºtfi Kerem ≈ûenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Madotto Andrea and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram√≠rez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M√°ty√°s Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Micha≈Ç Swƒôdrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Mi≈Çkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ram√≥n Risco Delgado and Rapha√´l Milli√®re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Th√©o Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Timothy Telleen-Lawton and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhao Xinran and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},<br />
								year    = {2022},<br />
								journal = {arXiv preprint arXiv: Arxiv-2206.04615}<br />
							  }
								
							</bibtext>
	
	
						<a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tense" target="_blank">Tense task</a>
						<br>
						<br>
						<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/google/BIG-bench?style=social">
						&emsp;
						<img alt="GitHub forks" src="https://img.shields.io/github/forks/google/BIG-bench?style=social">
						&emsp;
						<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LwiJwNYAAAAJ&citation_for_view=LwiJwNYAAAAJ:aqlVkmm33-oC" target="_blank"><img src="https://img.shields.io/badge/Citations-1855-lightgrey.svg?style=social&logo=Google Scholar"></a>
						</div>             
						<script language="JavaScript">hideblock('big_bib');hideblock('big_abs');</script>
				</td>
				</tr>

				<tr class="therow1 therow6" style="">
					<td width="15%" valign="center" align="center"><img src="images/aurora.png" alt="nthu" width="250" height="250"></td>
					</td>
					<td width="85%" style="padding:20px;vertical-align:middle">
						  <a class="tog" href="#aurora">
							<papertitle>Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order</papertitle>
						  </a>
						  <br>
						  <br>
							<strong>Diganta Misra</strong>,
							Multiple authors
						  <br>
						  <p></p>
						  <em>COLING 2025 (Industry Track)</em>
						  <br>
						  <div class="paper" id="aurora">
							<a href="https://arxiv.org/abs/2404.00399" target="_blank">paper</a> /
							  <a class="tog" href="javascript:toggleblock('aurora_abs')">abstract / 
							  <a class="tog" href="javascript:toggleblock('aurora_bib')">bibtex</a> 
							  <br>
							  <br>
							  <a href="https://huggingface.co/blog/mayank-mishra/aurora" target="_blank"><img alt="HuggingFace report" src="https://img.shields.io/badge/%F0%9F%A4%97-Report-yellow"></a>
							  <a href="https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407" target="_blank"><img alt="HuggingFace Models" src="https://img.shields.io/badge/%F0%9F%A4%97-Models-yellow"></a>
							  <a href="https://x.com/__z__9/status/1774965364301971849?s=20" target="_blank"><img alt="Twitter" src="https://img.shields.io/badge/Thread-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white" width="70"></a>
							  <p align="justify">
								<i id="aurora_abs">
									Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Aurora-M is rigorously evaluated across various tasks and languages, demonstrating robustness against catastrophic forgetting and outperforming alternatives in multilingual settings, particularly in safety evaluations.
								</i>
							  </p>
							 
							<bibtext xml:space="preserve" id="aurora_bib">
								@misc{nakamura2024auroram,
									title={Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order}, 
									author={Taishi Nakamura and Mayank Mishra and Simone Tedeschi and Yekun Chai and Jason T Stillerman and Felix Friedrich and Prateek Yadav and Tanmay Laud and Vu Minh Chien and Terry Yue Zhuo and Diganta Misra and Ben Bogin and Xuan-Son Vu and Marzena Karpinska and Arnav Varma Dantuluri and Wojciech Kusa and Tommaso Furlanello and Rio Yokota and Niklas Muennighoff and Suhas Pai and Tosin Adewumi and Veronika Laippala and Xiaozhe Yao and Adalberto Junior and Alpay Ariyak and Aleksandr Drozd and Jordan Clive and Kshitij Gupta and Liangyu Chen and Qi Sun and Ken Tsui and Noah Persaud and Nour Fahmy and Tianlong Chen and Mohit Bansal and Nicolo Monti and Tai Dang and Ziyang Luo and Tien-Tung Bui and Roberto Navigli and Virendra Mehta and Matthew Blumberg and Victor May and Huu Nguyen and Sampo Pyysalo},
									year={2024},
									eprint={2404.00399},
									archivePrefix={arXiv},
									primaryClass={cs.CL}
							  }
									
								</bibtext>  
								
						<a href="https://youtu.be/5PsRbCbpWg8?si=5R3rh7SvAemBBk9e" target="_blank"><img src="https://img.shields.io/badge/Munich NLP-red?style=for-the-badge&logo=youtube&logoColor=white"></a>
								<br>
								<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10634940790182863008" target="_blank"><img src="https://img.shields.io/badge/Citations-11-lightgrey.svg?style=social&logo=Google Scholar"></a>
								<br>
							</div>        
							<script language="JavaScript">hideblock('aurora_abs');hideblock('aurora_bib');</script>
					</td>
					</tr>

			<tr class="therow1 therow4" style="">
			<td width="15%" valign="center" align="center"><img src="images/app.drawio.png" alt="nthu" width="400" height="170"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#app2022">
                    <papertitle>APP: Anytime Progressive Pruning</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra<sup>*</sup></strong>,
				  <a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal<sup>*</sup></a>,
				<a href="https://tianlong-chen.github.io/about/" target="_blank">Tianlong Chen</a>,
				<a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Zhangyang Wang</a>,
				<a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a>
				  <br>
                  <p></p>
                  <em>DyNN workshop at ICML,2022</em>
				  <br>
				  <em>SNN, 2022</em>
				  <br>
				  <em>CLL workshop at ACML, 2022</em>
				  <br>
				  <em>SlowDNN workshop, 2023</em>
                  <div class="paper" id="app2022">
                    <a class="tog" href="https://github.com/landskape-ai/Progressive-Pruning" target="_blank">project</a> /
                    <a href="https://arxiv.org/abs/2204.01640" target="_blank">paper</a> /
					<a href="https://landskape.ai/publication/app/" target="_blank">webpage</a> /
					<a href="https://dynn-icml2022.github.io/posters/paper_22.pdf" target="_blank">poster</a> /
                      <a class="tog" href="javascript:toggleblock('app_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('app_bib')">bibtex</a>
                      <p align="justify">
                        <i id="app_abs">
                            With the latest advances in deep learning, there has been a lot of focus on the online learning paradigm due to its relevance in practical settings. Although many methods have been investigated for optimal learning settings in scenarios where the data stream is continuous over time, sparse networks training in such settings have often been overlooked. In this paper, we explore the problem of training a neural network with a target sparsity in a particular case of online learning: the anytime learning at macroscale paradigm (ALMA). We propose a novel way of progressive pruning, referred to as \textit{Anytime Progressive Pruning} (APP); the proposed approach significantly outperforms the baseline dense and Anytime OSP models across multiple architectures and datasets under short, moderate, and long-sequence training. Our method, for example, shows an improvement in accuracy of $\approx 7\%$ and a reduction in the generalization gap by $\approx 22\%$, while being $\approx 1/3$ rd the size of the dense baseline model in few-shot restricted imagenet training. We further observe interesting nonmonotonic transitions in the generalization gap in the high number of megabatches-based ALMA. The code and experiment dashboards can be accessed at \url{https://github.com/landskape-ai/Progressive-Pruning} and \url{https://wandb.ai/landskape/APP}, respectively.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="app_bib">
						@misc{misra2022app,<br />
							title={APP: Anytime Progressive Pruning},<br />
							author={Diganta Misra and Bharat Runwal and Tianlong Chen and Zhangyang Wang and Irina Rish},<br />
							year={2022},<br />
							eprint={2204.01640},<br />
							archivePrefix={arXiv},<br />
							primaryClass={cs.LG}<br />
						}
                            
                        </bibtext>
					
					<a href="https://bluejeans.com/playback/s/jFFH3X6y1UxAuLKW5kau2WRQHbMojKfpaySI6xGRQ56lqpaEmbdKsC9wBwWTJUag" target="_blank">NSL presentation</a> /
					<a href="https://youtu.be/GbHpaqwkgxE?t=924" target="_blank">MLC Research Jam #8</a> /
					<br>
					<a href="https://youtu.be/tXWcrwOwhFE?t=573" target="_blank">MLC Research Jam #9</a> /
					<a href="https://youtu.be/EZS2PzfyfXY" target="_blank">Continual AI Seminar</a>
					<br>
					<br>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/landskape-ai/Progressive-Pruning?style=social">
					&emsp;
					<img alt="GitHub Repo forks" src="https://img.shields.io/github/forks/landskape-ai/Progressive-Pruning?style=social">
					&emsp;
					<a href="https://wandb.ai/landskape/APP" target="_blank"><img src="https://img.shields.io/badge/Dashboard-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
					&emsp;
					<a href="https://www.semanticscholar.org/paper/APP%3A-Anytime-Progressive-Pruning-Misra-Runwal/473b6a20a44f0908845f00cacf7f4b50aaf3e616#citing-papers" target="_blank"><img src="https://img.shields.io/badge/Citations-2-lightgrey.svg?style=social&logo=Semantic Scholar"></a>
                    </div>              
                    <script language="JavaScript">hideblock('app_bib');hideblock('app_abs');</script>
            </td>
			</tr>

			<tr class="therow1 therow6" style="">
			<td width="15%" valign="center" align="center"><img src="images/cpal.png" alt="nthu" width="400" height="140"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#d2sparse2023">
                    <papertitle>$\mathcal{D}^2$-Sparse: Navigating the low data learning regime with sparse networks</papertitle>
                  </a>
                  <br>
				  	<strong>Diganta Misra</strong><sup>*</sup>,
					<a href="https://nolte.dev/" target="_blank">Niklas Nolte</a><sup>*</sup>,
					<a href="https://mila.quebec/en/person/sparsha-mishra/" target="_blank">Sparsha Mishra</a>,
					<a href="https://luuyin.com/" target="_blank">Lu Yin</a>
				  <br>
                  <p></p>
                  <em>PML4LRS @ ICLR, 2024 (<span style="color:rgb(200, 0, 255)">Oral</span>)</em>
				  <br>
                  <div class="paper" id="d2sparse2023">
					<a href="https://openreview.net/pdf/669997118d97828044eeb76f35d1c03304465f61.pdf" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('d2sparse2023_abs')">abstract</a> /
					  <a class="tog" href="javascript:toggleblock('d2sparse2023_bib')">bibtex</a>
                      <p align="justify">
                        <i id="d2sparse2023_abs">
                            Learning under constraints has been a fundamental avenue of research in deep learning since the advent of modern deep neural networks. In parallel to the upwards trajectory of scaling neural networks, one practical constraint that has embodied efficient deep learning has been that of sparsity. Unstructured weight sparsity has been the cornerstone of pioneering works in the space of pruning and lottery ticket hypothesis. In this paper, we propose \textbf{$\mathcal{D}^2$-Sparse}, a novel dual dynamic sparse learning system for low-data learning regime. Our paper combines two popular constraints in deep learning namely sparsity and low-data learning, often studied in disjoint paradigms, thus opening new directions of research in sparsity. $\mathcal{D}^2$-Sparse outperforms standard iterative pruning schema when coupled with standard deep networks in computer vision tasks like image classification and in natural language processing like code generation with no extra-overhead cost on inference. Compared to iterative pruning, on $\frac{1}{8}$-th total data budget, $\mathcal{D}^2$-Sparse achieves a $\approx$ 4% top-1 accuracy boost for ResNet-18 on the CIFAR-100 classification task. Further, we demonstrate the effectiveness of the proposed method in anytime learning scenarios and provide extensive analysis into evolution of sparse masks in $\mathcal{D}^2$-Sparse over the training process. Code, dashboard, and model weights will be open-sourced for public access upon acceptance.
                        </i>
                      </p>
					  <bibtext xml:space="preserve" id="d2sparse2023_bib">
						@inproceedings{misramathcal,
							title={$$\backslash$mathcal $\{$D$\}$\^{} 2$-Sparse: Navigating the low data learning regime with coupled sparse networks},
							author={Misra, Diganta and Nolte, Niklas and Mishra, Sparsha and Yin, Lu},
							booktitle={5th Workshop on practical ML for limited/low resource settings}
						  }
					  </bibtext>
                    </div>              
                    <script language="JavaScript">hideblock('d2sparse2023_abs');hideblock('d2sparse2023_bib');</script>
            </td>
			</tr>
			<br>
			<br>

				<tr class="therow1 therow4" style="">
					<td width="15%" valign="center" align="center"><img src="images/CVPR_VP.png" alt="nthu" width="320" height="450"></td>
					</td>
					<td width="85%" style="padding:20px;vertical-align:middle">
						  <a class="tog" href="#vp2023">
							<papertitle>Uncovering the Hidden Cost of Model Compression</papertitle>
						  </a>
						  <br>
						  <strong>Diganta Misra</strong><sup>*</sup>,
						  <a href="https://scholar.google.com/citations?user=4Z8ePskAAAAJ&hl=en" target="_blank">Muawiz Chaudhary</a><sup>*</sup>,
						  <a href="https://github.com/AGoyal0512" target="_blank">Agam Goyal</a><sup>*</sup>,
						  <a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a><sup>*</sup>,
						  <a href="https://sites.google.com/site/pinyuchenpage/home" target="_blank">Pin Yu Chen</a>
						  <br>
						  <p></p>
						  <em>PiV @ CVPR, 2024</em>
						  <br>
						  <div class="paper" id="vp2023">
							<a href="https://arxiv.org/abs/2308.14969" target="_blank">paper</a> /
							  <a class="tog" href="javascript:toggleblock('vp_abs')">abstract</a> /
							  <a class="tog" href="javascript:toggleblock('vp_bib')">bibtex</a>
							  <p align="justify">
								<i id="vp_abs">
									In the era of resource-intensive foundation models, efficient adaptation in downstream tasks has become paramount. Visual Prompting (VP), inspired by prompting in Large Language Models (LLMs), has emerged as a key transfer learning method in computer vision. Aligned with the growing significance of efficiency, research in model compression has become pivotal to alleviate the computational burden in both training and deploying over-parameterized neural networks. A key goal in model compression is the development of sparse models capable of matching or surpassing the performance of their over-parameterized, dense counterparts. While prior research has explored the impact of model sparsity on transfer learning, its effects on visual prompting-based transfer remain unclear. This study addresses this gap, revealing that model sparsity adversely affects the performance of visual prompting-based transfer, particularly in low-data-volume scenarios. Furthermore, our findings highlight the negative influence of sparsity on the calibration of downstream visual-prompted models. This empirical exploration calls for a nuanced understanding beyond accuracy in sparse settings, opening avenues for further research in Visual Prompting for sparse models.
								</i>
							  </p>
							  <bibtext xml:space="preserve" id="vp_bib">
								@article{misra2023reprogramming,
									title   = {Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets},
									author  = {Diganta Misra and Agam Goyal and Bharat Runwal and Pin Yu Chen},
									year    = {2023},
									journal = {arXiv preprint arXiv: 2308.14969}
								  }
								</bibtext>
							<a href="https://docs.google.com/presentation/d/1idZnvrOA-8ctimqn7l5kHwNUDNSWz9l_MqzB4RzC4P4/edit?usp=sharing" target="_blank">Cohere ForAI Lightning Talk</a> /
							<a href="https://docs.google.com/presentation/d/116Xgf0gVeYoU0zNSKpmwa-Rg0-zWjzMlFw7pcp63P3w/" target="_blank">Google Sparsity Reading Group Talk</a> /
							<a href="" target="_blank">MLC Research Jam 17</a>
							<br>
							<br>
								<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/landskape-ai/Reprogram_LT?style=social">
								&emsp;
								<img alt="GitHub forks" src="https://img.shields.io/github/forks/landskape-ai/Reprogram_LT?style=social">
								&emsp;
								<a href="https://wandb.ai/landskape/Reprogram-Sparse" target="_blank"><img src="https://img.shields.io/badge/Dashboard-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
								&emsp;
								<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18246231210916640798&as_sdt=5" target="_blank"><img src="https://img.shields.io/badge/Citations-7-lightgrey.svg?style=social&logo=Google Scholar"></a>
							</div>              
							<script language="JavaScript">hideblock('vp_abs');;hideblock('vp_bib');</script>
						</td>
					</tr>
	
				<tr class="therow1 therow2" style="">
				<td width="15%" valign="center" align="center"><img src="images/scole.drawio.png" alt="nthu" width="350" height="80"></td>
				</td>
				<td width="85%" style="padding:20px;vertical-align:middle">
					  <a class="tog" href="#scole2022">
						<papertitle>Challenging Common Assumptions about Catastrophic Forgetting</papertitle>
					  </a>
					  <br>
					  <a href="https://tlesort.github.io/" target="_blank">Timoth√©e Lesort</a>,
					  <a href="https://scholar.google.de/citations?user=mqLVUGgAAAAJ&hl=de" target="_blank">Oleksiy Ostapenko</a>,
					  <strong>Diganta Misra</strong>,
					  <a href="https://mila.quebec/personne/32484/" target="_blank">Md Rifat Arefin</a>,
					  <a href="https://scholar.google.com/citations?user=IwBx73wAAAAJ&hl=fr" target="_blank">Pau Rodriguez</a>,
					  <a href="http://www.cs.toronto.edu/~lcharlin/" target="_blank">Laurent Charlin</a>,
					  <a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a>
					  <br>
					  <p></p>
					  <em>CoLLAs workshop, 2022</em>
					  <br>
					  <em>CoLLAs, 2023</em>
					  <div class="paper" id="scole2022">
						  <a href="https://arxiv.org/abs/2207.04543" target="_blank">paper</a> /
						  <a class="tog" href="javascript:toggleblock('scole_abs')">abstract</a> /
						  <a class="tog" href="javascript:toggleblock('scole_bib')">bibtex</a>
						  <p align="justify">
							<i id="scole_abs">
								Standard gradient descent algorithms applied to sequences of tasks are known to produce catastrophic forgetting in deep neural networks. When trained on a new task in a sequence, the model updates its parameters on the current task, forgetting past knowledge. This article explores scenarios where we scale the number of tasks in a finite environment. Those scenarios are composed of a long sequence of tasks with reoccurring data. We show that in such setting, stochastic gradient descent can learn, progress, and converge to a solution that according to existing literature needs a continual learning algorithm. In other words, we show that the model performs knowledge retention and accumulation without specific memorization mechanisms. We propose a new experimentation framework, SCoLe (Scaling Continual Learning), to study the knowledge retention and accumulation of algorithms in potentially infinite sequences of tasks. To explore this setting, we performed a large number of experiments on sequences of 1,000 tasks to better understand this new family of settings. We also propose a slight modifications to the vanilla stochastic gradient descent to facilitate continual learning in this setting. The SCoLe framework represents a good simulation of practical training environments with reoccurring situations and allows the study of convergence behavior in long sequences. Our experiments show that previous results on short scenarios cannot always be extrapolated to longer scenarios.
							</i>
						  </p>
	
						  <bibtext xml:space="preserve" id="scole_bib">
							@article{lesort2022scaling,<br />
								title   = {Scaling the Number of Tasks in Continual Learning},<br />
								author  = {Timoth√©e Lesort and Oleksiy Ostapenko and Diganta Misra and Md Rifat Arefin and Pau Rodr√≠guez and Laurent Charlin and Irina Rish},<br />
								year    = {2022},<br />
								journal = {arXiv preprint arXiv: Arxiv-2207.04543}<br />
							}
							</bibtext>
	
						<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9761510814774574611,6151901743655049627&as_sdt=5" target="_blank"><img src="https://img.shields.io/badge/Citations-23-lightgrey.svg?style=social&logo=Google Scholar"></a> 
	
						<br>
						</div>              
						<script language="JavaScript">hideblock('scole_abs');;hideblock('scole_bib');</script>
				</td>
				</tr>


				

				<tr class="therow1 therow4" style="">
					<td width="15%" valign="center" align="center"><img src="images/cvpr_mamba.png" alt="nthu" width="320" height="280"></td>
					</td>
					<td width="85%" style="padding:20px;vertical-align:middle">
						  <a class="tog" href="#2024mamba">
							<papertitle>On the low-shot transferability of [V]-Mamba</papertitle>
						  </a>
						  <br>
						  <strong>Diganta Misra</strong><sup>*</sup>,
						  <a href="https://jaygala24.github.io/" target="_blank">Jay Gala</a><sup>*</sup>,
						  <a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a>
						  <br>
						  <p></p>
						  <em>PiV @ CVPR, 2024</em>
						  <br>
						  <div class="paper" id="mamba2024">
							<a href="https://arxiv.org/abs/2403.10696" target="_blank">paper</a> /
							  <a class="tog" href="javascript:toggleblock('mamba_abs')">abstract</a> /
							  <a class="tog" href="javascript:toggleblock('mamba_bib')">bibtex</a>
							  <p align="justify">
								<i id="mamba_abs">
									The strength of modern large-scale neural networks lies in their ability to efficiently adapt to new tasks with few examples. Although extensive research has investigated the transferability of Vision Transformers (ViTs) to various downstream tasks under diverse constraints, this study shifts focus to explore the transfer learning potential of [V]-Mamba. We compare its performance with ViTs across different few-shot data budgets and efficient transfer methods. Our analysis yields three key insights into [V]-Mamba's few-shot transfer performance: (a) [V]-Mamba demonstrates superior or equivalent few-shot learning capabilities compared to ViTs when utilizing linear probing (LP) for transfer, (b) Conversely, [V]-Mamba exhibits weaker or similar few-shot learning performance compared to ViTs when employing visual prompting (VP) as the transfer method, and (c) We observe a weak positive correlation between the performance gap in transfer via LP and VP and the scale of the [V]-Mamba model. This preliminary analysis lays the foundation for more comprehensive studies aimed at furthering our understanding of the capabilities of [V]-Mamba variants and their distinctions from ViTs.
								</i>
							  </p>
							  <bibtext xml:space="preserve" id="mamba_bib">
							  @misc{misra2024lowshot,
								title={On the low-shot transferability of [V]-Mamba}, 
								author={Diganta Misra and Jay Gala and Antonio Orvieto},
								year={2024},
								eprint={2403.10696},
								archivePrefix={arXiv},
								primaryClass={cs.CV}
						  }
								</bibtext>

								<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16214979971523932598" target="_blank"><img src="https://img.shields.io/badge/Citations-1-lightgrey.svg?style=social&logo=Google Scholar"></a>
							<br>
							</div>              
							<script language="JavaScript">hideblock('mamba_abs');hideblock('mamba_bib');</script>
					</td>
					</tr>

					<tr class="therow1 therow6" style="">
						<td width="15%" valign="center" align="center"><img src="https://miro.medium.com/v2/resize:fit:1400/1*LeHoazxpdU3tyARAHVixsw.jpeg" alt="nthu" width="250" height="120"></td>
						</td>
						<td width="85%" style="padding:20px;vertical-align:middle">
							  <a class="tog" href="#moegflow2023">
								<papertitle>Mitigating Mode Collapse in Sparse Mixture of Experts</papertitle>
							  </a>
							  <br>
							  <a href="https://nizarislah.github.io/" target="_blank">Nizar Islah</a>, 
							  <strong>Diganta Misra</strong>,
							  <a href="https://scholar.google.gr/citations?user=HIempWQAAAAJ&hl=en" target="_blank">Timothy Nest</a>, 
							  <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>
							  <br>
							  <p></p>
							  <em>NeurIPS New in ML workshop, 2023</em>
							  <br>
							  <div class="paper" id="moegflow2023">
								  <a class="tog" href="javascript:toggleblock('moegflow_abs')">abstract</a>
								  <p align="justify">
									<i id="moegflow_abs">
										The recent success of Sparse Mixture-of-Experts (SMoEs) models has sparked renewed interest in routed networks in deep learning. A prominent aspect of the SMoE is the scaling of the number of total parameters in a model, effectively increasing capacity while keeping computation costs similar to dense models. Yet, these models pose optimization challenges as inputs are routed discretely to experts in each layer. Often, a regularization term is added to the loss function to penalize the imbalanced selection of experts. We aim to demonstrate that the heuristic regularization strategies used in recent SMoEs, while successful in some tasks, have significant limitations which we aim to address. In multi-domain or multi-task settings, without explicit knowledge of the task or domain, the network will suffer from a mode collapse-performance tradeoff, in which some experts will receive significantly less training signal, or performance on some tasks will suffer. Second, we derive a theoretical basis of the various routing functions, with entropy-maximization as a common objective. Third, we will demonstrate a first application of Generative Flow Networks (GFlowNets) to SMoEs, with a state, policy, and action space,  represented at a particular layer of the model by the input, routing network, and sampling from expert probabilities, respectively. We aim to show that SMoEs trained with the Trajectory Balance objective from GFlowNet literature can achieve competitive performance with state of the art routing methods, such as Switch Transformer, and suffer less from expert collapse in multi-task (NYUv2, Pascal-Context) and multi-domain (Omniglot) settings. This work lays some foundations for further exploration of theoretically motivated approaches to routing in sparse MoEs.
									</i>
								  </p>
								</div>              
								<script language="JavaScript">hideblock('moegflow_abs');</script>
						</td>
						</tr>
		

			<tr class="therow1 therow4" style="">
			<td width="15%" valign="center" align="center"><img src="images/code.png" alt="nthu" width="410" height="200"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#code2023">
                    <papertitle>Pruning CodeBERT for Improved Code-to-Text Efficiency</papertitle>
                  </a>
                  <br>
				  <a href="https://www.linkedin.com/in/alex-gu-8b7664175" target="_blank">Alex Gu</a>,
				  <a href="https://www.linkedin.com/in/riasonecha" target="_blank">Ria Sonecha</a>,
				  <a href="https://www.linkedin.com/in/saaketh-vedantam" target="_blank">Saaketh Vedantam</a>,
				  <a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>,
                  <strong>Diganta Misra</strong>
				  <br>
                  <p></p>
                  <em>SNN workshop at ICLR, 2023</em>
				  <br>
                  <div class="paper" id="code2023">
					<a href="data/snn.pdf" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('code_abs')">abstract</a> /
					  <a class="tog" href="javascript:toggleblock('code_bib')">bibtex</a>
                      <p align="justify">
                        <i id="code_abs">
                            The size and prevalence of large language models (LLMs) make them an apt target for model compression. Most LLMs consist of a Transformer encoder and decoder, which each have 6 to 12 layers of multiheaded self-attention blocks, along with fully connected layers. This results in a large number of parameters, making them quite expensive to train and query. Our work focuses on finding techniques to prune CodeBERT, a specific LLM trained to work multimodally between text and code. We explore the effects of structured and unstructured magnitude pruning on the encoder layers of CodeBERT, evaluating on the task of generating natural language comments from a piece of Ruby code.
                        </i>
                      </p>
					  <bibtext xml:space="preserve" id="code_bib">
					  @article{gupruning,
						title={Pruning CodeBERT for Improved Code-to-Text Efficiency},
						author={Gu, Alex and Sonecha, Ria and Vedantam, Saaketh and Runwal, Bharat and Misra, Diganta}
					  }
					  </bibtext>
					<a href="data/Preliminary Pruning on Language Models for Code.pptx.pdf" target="_blank">poster</a>
                    </div>              
                    <script language="JavaScript">hideblock('code_abs');hideblock('code_bib');</script>
            </td>
			</tr>

			<tr class="therow1 therow6" style="">
				<td width="15%" valign="center" align="center"><img src="images/SPIRITmethod.png" alt="nthu" width="200" height="240"></td>
				</td>
				<td width="85%" style="padding:20px;vertical-align:middle">
					  <a class="tog" href="#spirit2023">
						<papertitle>SPIRIT: Zero Shot Information Retrieval Domain Transfer with
							Soft Prompts</papertitle>
					  </a>
					  <br>
					  <a href="https://ethankim00.github.io./" target="_blank">Ethan Kim</a>,
					  <strong>Diganta Misra</strong>
					  <br>
					  <p></p>
					  <em>Preprint</em>
					  <br>
					  <div class="paper" id="spirit2023">
						  <a href="data/sigir.pdf" target="_blank">paper</a> /
						  <a class="tog" href="javascript:toggleblock('spirit_abs')">abstract</a>
						  <p align="justify">
							<i id="spirit_abs">
								Dense information retrieval yields strong in-domain performance,
								but often struggles with out-of-domain generalization, lagging be-
								hind unsupervised methods. Retrieval tasks can vary across a num-
								ber of dimensions including domain, query intent, and language.
								Using a single dense retrieval model for all tasks often underper-
								forms lexical methods such as BM25. For practical information
								retrieval systems, it is expensive to deploy a different model for
								each task. Therefore, our motivation is to develop a cheap and
								effective information retrieval model that maintains strong per-
								formance across different domains while easily adapting to any
								new domain. Other approaches to domain transfer in information
								retrieval rely on large auxiliary language models or datasets and
								create a separate model for each task. In this work, we develop a
								method utilizing prompt tuning to efficiently adapt dense retrievers
								with a minimal amount of additional computation. By combining
								models trained on a variety of different domains, we can effectively
								boost performance on a target task in a new domain. Specifically,
								we train dense retrieval models using prompt tuning on a large
								number of information retrieval tasks across diverse domains and
								types of query intents. To adapt to a new domain, we create new
								prompt embeddings by averaging the prompt embeddings from a
								set of source tasks selected in an unsupervised manner. We evaluate
								zero-shot transfer performance across a wide variety of information
								retrieval domains and show competitive performance while lever-
								aging a minimal amount of compute. Notably, our SPIRIT method
								achieves while being extremely lightweight and practical to deploy
								in production.
							</i>
						  </p>
						</div>              
						<script language="JavaScript">hideblock('spirit_abs');</script>
				</td>
				</tr>

			<tr class="therow1 therow2" style="">
			<td width="15%" valign="center" align="center"><img src="images/ga.jpg" alt="nthu" width="330" height="150"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#ga">
                    <papertitle>Genetic Algorithm Optimized Inkjet Printed Electromagnetic Absorber on Paper Substrate</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>,
				  Rahul Pelluri,
				  <a href="https://scholar.google.com/citations?user=iK_wjZsAAAAJ&hl=en" target="_blank">Vijay Kumar Verma</a>,
				  <a href="https://scholar.google.com/citations?user=iGAvOmEAAAAJ&hl=en" target="_blank">Bhargav Appasani</a>,
				  Nisha Gupta
				  <br>
                  <p></p>
                  <em>IEEE AESPC, 2018</em>
                  <div class="paper" id="ga2018">
                    <a href="data/misra2018.pdf" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('ga_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('ga_bib')">bibtex</a>
                      <p align="justify">
                        <i id="ga_abs">
                            Printable electronics based electromagnetic absorbers are receiving increasing attention of the electromagnetic community because of their unprecedented advantages. This paper presents the design of printable electromagnetic absorbers for the X band. The design of the absorber is optimized using the Genetic Algorithm (GA) to enhance the absorptivity and the absorption bandwidth. The design involves the placement of several square-shaped conductive ink at optimal locations on the paper substrate such that desired absorption characteristics are obtained. Simulations are carried out using the HFSS simulation software. The optimized structure offers an absorptivity of more than 90% in the X band thereby proving to be a viable solution for stealth applications.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="ga_bib">
						@inproceedings{misra2018genetic, <br />
						title={Genetic Algorithm Optimized Inkjet Printed Electromagnetic Absorber on Paper Substrate}, <br />
						author={Misra, Diganta and Pelluri, Rahul and Verma, Vijay Kumar and Appasani, Bhargav and Gupta, Nisha}, <br />
						booktitle={2018 International Conference on Applied Electromagnetics, Signal Processing and Communication (AESPC)}, <br />
						volume={1}, <br />
						pages={1--3}, <br />
						year={2018}, <br />
						organization={IEEE} <br />
						}
                            
                        </bibtext>
						<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1834772988619324668" target="_blank"><img src="https://img.shields.io/badge/Citations-1-lightgrey.svg?style=social&logo=Google Scholar"></a>
					</div>              
                    <script language="JavaScript">hideblock('ga_bib');hideblock('ga_abs');</script>
            </td>
		</tr>


		<tr class="therow1 therow2" style="">
			<td width="15%" valign="center" align="center"><img src="https://www.ajnr.org/content/ajnr/27/6/1373/F4.large.jpg" alt="nthu" width="250" height="120"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#wilson">
                    <papertitle>Large-Scale Meta-Analysis of Genes Encoding Pattern in Wilson‚Äôs Disease &emsp14; (<span style="color:rgb(81, 0, 255)">Best Paper Award</span>)</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>,
				  <a href="https://csed.thapar.edu/facultydetails/MTU0Ng==" target="_blank">Anurag Tiwari</a>,
				  <a href="https://sites.google.com/site/dramritachaturvedi/home" target="_blank"> Amrita Chaturvedi</a>
				  <br>
                  <p></p>
                  <em>Springer IC4S, 2018</em>
                  <div class="paper" id="wilson2019">
                    <a href="https://link.springer.com/chapter/10.1007/978-981-13-6861-5_34" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('wilson_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('wilson_bib')">bibtex</a>
                      <p align="justify">
                        <i id="wilson_abs">
                            In this paper, we propose an unsupervised learning approach with an objective to understand gene expressions for analysis of Wilson‚Äôs disease in the liver of Mus musculus organisms. We proceeded to obtain the best parameters for cluster division to correctly classify gene expression sets so as to capture the effect and characteristics of the disease in the genome levels of the organisms in the best possible way. The clustering proved beneficial in capturing the correct genetic analogy of Wilson‚Äôs disease. Analytical experiments were carried out using various clustering algorithms and were evaluated using performance metrics including silhouette score analysis and Calinski‚ÄìHarabasz index.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="wilson_bib">
						@inproceedings{misra2019large,
							title={Large-Scale Meta-Analysis of Genes Encoding Pattern in Wilson‚Äôs Disease},
							author={Misra, Diganta and Tiwari, Anurag and Chaturvedi, Amrita},
							booktitle={Advances in Computer Communication and Computational Sciences: Proceedings of IC4S 2018},
							pages={389--400},
							year={2019},
							organization={Springer}
						  }
                            
                        </bibtext>
					</div>              
                    <script language="JavaScript">hideblock('wilson_bib');hideblock('wilson_abs');</script>
            </td>
		</tr>

		<tr class="therow1 therow2" style="">
		<td width="15%" valign="center" align="center"><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12145-019-00434-8/MediaObjects/12145_2019_434_Fig8_HTML.png" alt="nthu" width="200" height="200"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#cc">
                    <papertitle>Convoluted Cosmos: Classifying Galaxy Images Using Deep Learning</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>,
				  <a href="https://scholar.google.co.in/citations?user=166bepcAAAAJ&hl=en" target="_blank">Sachi Nandan Mohanty</a>,
				  <a href="https://scholar.google.co.uk/citations?user=gt9wdUwAAAAJ&hl=en&oi=ao" target="_blank">Mohit Agarwal</a>,
				  <a href="https://scholar.google.co.uk/citations?user=ZYXq9xoAAAAJ&hl=en&oi=ao" target="_blank">Suneet K Gupta</a>
				  <br>
                  <p></p>
                  <em>Springer ICDMAI, 2019 (Proceedings of the AISC)</em>
                  <div class="paper" id="ga2018">
                    <a href="https://link.springer.com/chapter/10.1007/978-981-32-9949-8_40#chapter-info" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('cc_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('cc_bib')">bibtex</a>
                      <p align="justify">
                        <i id="cc_abs">
                            In this paper, a deep learning-based approach has been developed to classify the images of galaxies into three major categories, namely, elliptical, spiral, and irregular. The classifier successfully classified the images with an accuracy of 97.3958%, which outperformed conventional classifiers like Support Vector Machine and Naive Bayes. The convolutional neural network architecture involves one input convolution layer having 16 filters, followed by 4 hidden layers, 1 penultimate dense layer, and an output Softmax layer. The model was trained on 4614 images for 200 epochs using NVIDIA-DGX-1 Tesla-V100 Supercomputer machine and was subsequently tested on new images to evaluate its robustness and accuracy.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="cc_bib">
						@incollection{misra2020convoluted,<br />
							title={Convoluted cosmos: classifying galaxy images using deep learning},<br />
							author={Misra, Diganta and Mohanty, Sachi Nandan and Agarwal, Mohit and Gupta, Suneet K},<br />
							booktitle={Data Management, Analytics and Innovation},<br />
							pages={569--579},<br />
							year={2020},<br />
							publisher={Springer}<br />
						  }

					</bibtext>
					<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15150003569569138477" target="_blank"><img src="https://img.shields.io/badge/Citations-13-lightgrey.svg?style=social&logo=Google Scholar"></a>
				</div>              
				<script language="JavaScript">hideblock('cc_bib');hideblock('cc_abs');</script>
		</td>
	</tr>
          
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research under progress</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			
			<td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/26BRzANyVb84VSQq4" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="pred">
				<papertitle>Dynamic Sparse Upcycling</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				Fanmin Shi (University of Amsterdam),
				<a href="https://github.com/NizarIslah" target="_blank">Nizar Islah (Mila)</a>
				<br>
				<p></p>
			</td>
			</tr>


        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Open Source Frameworks & Projects</heading> &emsp; <!-- Place this tag where you want the button to render. -->
			<a class="github-button" href="https://github.com/sponsors/digantamisra98" data-color-scheme="no-preference: light; light: light; dark: light;" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @digantamisra98 on GitHub">Sponsor</a>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <td width="25%" valign="center" align="center"><img src="images/avalanche.png" alt="nthu" width="180" height="160"></td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a class="tog" href="https://github.com/ContinualAI/avalanche" target="_blank">
              <papertitle>Avalanche: an End-to-End Library for Continual Learning</papertitle>
            </a>
            <br>
            Dec'20 - Oct'22
            <br>
            <br>
            I am an active lead maintainer of the Reproducible Continual Learning framework by Avalanche and
			also actively work on the evaluation framework of Avalanche mainly in the direction of integration of Weights & Biases API. 
            <br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/continualai/avalanche?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/continualai/avalanche?style=social">
			&emsp;
			<a href="https://avalanche-api.continualai.org/" target="_blank"><img src="https://img.shields.io/badge/docs-passing?style=social&logo=Read the Docs"></a>
            <p></p>
            
          </td>
        </tr>

		<td width="25%" valign="center" align="center"><img src="images/echo.png" alt="nthu" width="160" height="170"></td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a class="tog" href="https://github.com/digantamisra98/Echo" target="_blank">
              <papertitle>Echo</papertitle>
            </a>
            <br>
            Jun'19 - Jun'22
            <br>
            <br>
			Echo is an OSS deep learning package with support for TensorFlow, PyTorch and MegEngine, containing novel validated methods, components and building blocks used in deep learning.
            <br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/echo?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/echo?style=social">
			&emsp;
			<a href="https://pypi.python.org/pypi/echoAI" target="_blank"><img src="https://img.shields.io/pypi/dm/echoAI.svg?style=social&logo=PyPI"></a>
			&emsp;
			<a href="https://xa9ax.gitbook.io/echo/" target="_blank"><img src="https://img.shields.io/badge/docs-passing?style=social&logo=Read the Docs"></a>
            <p></p>
            
          </td>
        </tr>

		<td width="25%" valign="center" align="center"><img src="images/evonorm.png" alt="nthu" width="250" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/digantamisra98/EvoNorm" target="_blank">
			<papertitle>Evonorm</papertitle>
			</a>
			<br>
			Apr'20
			<br>
			<br>
			Created the most popular open source reimplementation of <a href="https://arxiv.org/abs/2004.02967" target="_blank">Evolving Normalization-Activation Layers</a> by Liu. et. al.
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/evonorm?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/evonorm?style=social">
			<p></p>
		</td>
		</tr>

		<td width="25%" valign="center" align="center"><img src="https://blog.paperspace.com/content/images/size/w1600/2020/09/eca_module.jpg" alt="eca" width="250" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET" target="_blank">
			<papertitle>ECANets</papertitle>
			</a>
			<br>
			Jan'21
			<br>
			<br>
			Reproduced the CVPR 2020 paper: <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.html">ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</a> for the ML Reproducibility Challenge 2020. Integrated with <a href="https://wandb.ai">Weights & Biases</a>.
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/Reproducibilty-Challenge-ECANET?style=social">
			&emsp;
			<!---<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/Reproducibilty-Challenge-ECANET?style=social">
			&emsp;--->
			<a href="https://blog.paperspace.com/attention-mechanisms-in-computer-vision-ecanet/" target="_blank"><img src="https://img.shields.io/badge/Blog-Paperspace?style=social&logo=data:image/png;base64,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"></a>
			&emsp;
			<a href="https://wandb.ai/diganta/ECANet-sweep/reports/ECA-Net-Efficient-Channel-Attention-for-Deep-Convolutional-Neural-Networks--VmlldzozODU0NTM" target="_blank"><img src="https://img.shields.io/badge/Report-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
			&emsp;
			<a href="https://mybinder.org/v2/gh/digantamisra98/Reproducibilty-Challenge-ECANET/HEAD" target="_blank"><img src="https://mybinder.org/badge_logo.svg"></a>
			<p></p>
		</td>
		</tr>

		<td width="25%" valign="center" align="center"><img src="images/BIG-bench.png" alt="nthu" width="120" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/google/BIG-bench" id="Big-Bench" target="_blank">
			<papertitle>Big Bench</papertitle>
			</a>
			<br>
			Aug'21
			<br>
			<br>
			Our <a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tense" target="_blank">fine grained tense modification task</a> was accepted to <a href="https://github.com/google/BIG-bench" target="_blank">Google's Big Bench</a> for testing large LMs. In collaboration with <a href="https://in.linkedin.com/in/mukundvarmat" target="_blank">Mukund Varma T.</a>
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/google/big-bench?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/google/big-bench?style=social">
			<p></p>

		</td>
		</tr>

		<td width="25%" valign="center" align="center"><img src="images/download.jpeg" alt="nthu" width="120" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/ontocord/MDEL" id="MDEL" target="_blank">
			<papertitle>Aurora-M</papertitle>
			</a>
			<br>
			April'23 - Feb'24
			<br>
			<br>
			I am currently the lead for the modelling part of the <i>Multi-Domain Expert Layers (<b>MDEL</b>) Training: How to increase knowledge without breaking the bank?</i> as a collaborative effort co-ordinated by <a href="https://www.ontocord.ai/" target="_blank">Ontocord AI</a> wherein my team is working on different aspects of architecture design and training of the MDEL model on SUMMIT supercomputer cluster as part of the INCITE allocation.
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ontocord/MDEL?style=social">
			&emsp;
			<a href="https://huggingface.co/blog/mayank-mishra/aurora" target="_blank"><img alt="HuggingFace report" src="https://img.shields.io/badge/%F0%9F%A4%97-Report-yellow"></a>
			<p></p>

		</td>
		</tr>

		<td width="25%" valign="center" align="center"><img src="https://images.squarespace-cdn.com/content/v1/64039b76ece80729ee1864ec/e16df7f8-dcca-4530-aa83-fecd5ea14e81/new_logo.png" alt="nthu" width="120" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://www.ontocord.ai/" id="SafeLMM" target="_blank">
			<papertitle>SafeLMM</papertitle>
			</a>
			<br>
			Sept'24 - Present
			<br>
			<br>
			I currently work as one of the primary researchers in the synthetic data generation pipeline for subsequent training of a safe (under EU AI Safety Act) and reliable LMM. Our project is generously supported by a compute grant from <a href="https://www.cineca.it/en" target="_blank">CINECA</a> utilizing the <a href="https://leonardo-supercomputer.cineca.eu/" target="_blank">Leonardo Supercomputer</a>.
			<p></p>

		</td>
		</tr>


        </tbody></table>

		<br>
		<br>


		<table width="100%" border="0" cellspacing="15" cellpadding="10">
		<a id="education"><heading>&nbsp;&nbsp;&nbsp;Education</heading></a>

		<tr>
			<!-- <td width="15%" valign="center" align="center"><img src="images/udem.jpeg" alt="nthu" width="80" height="80"></td> -->
			<td width="15%" valign="center" align="center"><img src="images/mpi.png" alt="nthu" width="300" height="80"></td>
			<td width="85%" valign="center">
			<p>
				<span><strong>IMPRS-IS + ELLIS + Amazon Science Hub PhD Fellow</strong></span><span style="float:right">September 2024 - Present</span> <br>
				<em><a href="https://is.mpg.de/" target="_blank">Max Planck Institute for Intelligent Systems (MPI-IS)</a> + <a href="https://institute-tue.ellis.eu/" target="_blank">ELLIS Institute T√ºbingen</a> + <a href="https://tuebingen.ai/" target="_blank">T√ºbingen AI Center</a></em>
				<br>
				Advisors: <a href="http://orvi.altervista.org/" target="_blank">Antonio Orvieto</a> and <a href = "https://people.epfl.ch/volkan.cevher?lang=en" target="_blank">Volkan Cevher</a>.
				<br>
				T√ºbingen, Germany
				<br>
			</p>
			</td>
		</tr>
		
		
		
		
		
		<tr>
			<td width="15%" valign="center" align="center"><img src="images/mila.png" alt="nthu" width="112" height="130"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong><a href="https://mila.quebec/en/person/diganta-misra/">Masters in Machine Learning</a></strong></span><span style="float:right">September 2021 - August 2024</span> <br>
				<em><a href="https://mila.quebec/" target="_blank">Montr√©al Institute of Learning Algorithms (Mila)</a></em>
				<br>
				Advisor: <a href="https://sites.google.com/view/irinalab/about?authuser=0" target="_blank">Professor Irina Rish</a>
				<br>
				Montr√©al, Canada
				<br>
				Thesis: <a href="https://umontreal.scholaris.ca/items/d5e1eaee-96cc-4b49-b61a-07f20c29c340" target="_blank">Learning under constraints</a>
				<br>
			</p>
			</td>
		</tr>

		<tr>
			<!-- <td width="15%" valign="center" align="center"><img src="images/udem.jpeg" alt="nthu" width="80" height="80"></td> -->
			<td width="15%" valign="center" align="center">
				<figure class="half" style="display:flex">
					<img src="images/udem.jpeg" style="width:80px"> &emsp;
					<img src="https://www-labs.iro.umontreal.ca/~lapalme/DIRO_LOGO/Print/15_DIRO_LOGO_CMYK.jpg" width="100" height="20">
			</figure></td>
			<td width="85%" valign="center">
			<p>
				<span><strong>Masters in Informatique (MSc CS)</strong></span><span style="float:right">September 2021 - August 2024</span> <br>
				<em><a href="https://diro.umontreal.ca/accueil/" target="_blank">University of Montr√©al</a></em>
				<br>
				Advisor: <a href="https://sites.google.com/view/irinalab/about?authuser=0" target="_blank">Professor Irina Rish</a>
				<br>
				Montr√©al, Canada
				<br>
				Thesis: <a href="https://umontreal.scholaris.ca/items/d5e1eaee-96cc-4b49-b61a-07f20c29c340" target="_blank">Learning under constraints</a>
				<br>
			</p>
			</td>
		</tr>
		
		<tr>
			<td width="15%" valign="center" align="center"><img src="https://www.easytourz.com/uploads/Businesslogo/1541135407.png" alt="nthu" width="130" height="100"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Bachelors of Technology (B.Tech) in EEE</strong></span><span style="float:right">Jun. 2016 - May. 2020</span> <br>
				<em><a href="https://kiit.ac.in/" target="_blank">Kalinga Institute of Industrial Technology (KIIT)</a></em>
				<br>
				Advisor: <a href="https://scholar.google.com/citations?user=iGAvOmEAAAAJ&hl=en" target="_blank">Asst. Prof. Dr. Bhargav Appasani</a>
				<br>
				Bhubaneswar, India
			</p>
			</td>
		</tr>
		</table>


		<table width="100%" border="0" cellspacing="15" cellpadding="10">
			<heading>&nbsp;&nbsp;&nbsp;Internships and Exchange Programs</heading>
	
			<tr>
				<td width="15%" valign="center" align="center"><img src="https://yt3.googleusercontent.com/ytc/AOPolaSNjGD-f4sey3KRAFv43i50-1To4IQ0WLv_cHHrFA=s900-c-k-c0x00ffffff-no-rj" alt="nthu" width="120" height="120"></td>
				<td width="85%" valign="top">
				<p>
					<span><strong>Data Science Intern</strong></span><span style="float:right">Jun. 2018 - Feb. 2019</span> <br>
					<em><a href="https://cdri.res.in/" target="_blank">CSIR-CDRI</a></em>
					<br>
					<br>
					During this internship, I was involved in building the analytical pipeline, data collection, pre-processing of data, cleaning of data, Geo-spatial Analysis of data and Document writing for the project on understanding demographics of Venture Capital and
					Early Seed Investments. As a part of a team of three, I was advised and mentored by <a href="http://brainnart.com/" target="_blank">Dr. Sukant Khurana</a>.
					<br>
					<br>
					Remote
				</p>
				</td>
			</tr>
	
			<tr>
				<td width="15%" valign="center" align="center"><img src="images/kgp.png" alt="nthu" width="120" height="120"></td>
				<td width="85%" valign="top">
				<p>
					<span><strong>Summer Intern</strong></span><span style="float:right">May. 2018 - Jun. 2018</span> <br>
					<em><a href="http://www.iitkgp.ac.in/" target="_blank">IIT-Kharagpur</a></em>
					<br>
					<br>
					Studied basic algorithmic techniques using functional programming languages - Lisp
					and Prolog under the guidance of <a href="http://www.iitkgp.ac.in/department/MA/faculty/ma-pawan#resp-tab1" target="_blank">Assc. Prof. Pawan Kumar</a>.
					<br>
					<br>
					Kharagpur, India
				</p>
				</td>
			</tr>
	
			<tr>
				<td width="15%" valign="center" align="center">
					<figure class="half" style="display:flex">
						<img src="images/bangkok.png" style="width:80px"> &emsp;
						<img src="https://i.pinimg.com/originals/b7/5c/10/b75c10250bc8eab4ba2b389ef75772fe.png" style="width:80px">
				</figure></td>
				<!-- <td width="15%" valign="center" align="center"><img src="images/bangkok.png" alt="nthu" width="120" height="120"></td> -->
				<td width="85%" valign="top">
				<p>
					<span><strong>Summer Exchange Intern</strong></span><span style="float:right">Jun. 2017 - Aug. 2017</span> <br>
					<em><a href="https://www.bu.ac.th/en/international-programs" target="_blank">Bangkok University</a></em>
					<br>
					<br>
					Served as a primary instructor for cultural engagements along with teaching basic
					english and computer science to primary grade students at RangsonWittaya School,
					Nakhon Sawan under the <a href="https://aiesec.org/" target="_blank">AIESEC</a> SDG #4 programme. Was also part of culture
					exchange, entrepreneurship and social service programs at Bangkok University
					<br>
					<br>
					Bangkok, Thailand
				</p>
				</td>
			</tr>
			</table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Initiatives and Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%" valign="center" align="center"><img src="images/nma1.jpeg" height="80" width="160"></td>
            <td width="75%" valign="center">
                <a href="https://academy.neuromatch.io/" target="_blank">
                    <papertitle>NeuroMatch Academy</papertitle>
                </a>
                <br>
                <br>
				I was responsible for developing the content for the Strategies section in the Continual Learning lecture of the Deep Learning Cohort of Neuromatch Academy 2021.
              
            </td>
          </tr>
          <tr>
              <td style="padding:20px;width:25%" valign="center" align="center"><img src="images/wandb.jpeg" height="120" width="120"></td>
              <td width="75%" valign="center">
                  <a href="https://wandb.ai/site/reproducibility-challenge" target="_blank">
                      <papertitle>W&B ML Reproducibility Challenge</papertitle>
                  </a>
                  <br>
                  <br>
                  I was the lead organizer of the W&B MLRC 2021 where I actively supported our challenge participants. Our mission of organizing this challenge was 
																		to make machine learning research reproducible, transparent and accessible to everyone. This initiative was also supported by our W&B MLRC Grant of $500 for each participant.
                
              </td>
            </tr>
			

			<tr>
			<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/4f/%C3%89cole_Polytechnique_de_Montr%C3%A9al_Logo.svg/1200px-%C3%89cole_Polytechnique_de_Montr%C3%A9al_Logo.svg.png" height="140" width="150"></td>
			<td width="75%" valign="center">
				<a href="https://www.polymtl.ca/programmes/cours/iatech-probabilistes-et-dapprentissage" target="_blank">
					<papertitle>INF8225: Probabilistic Learning</papertitle>
				</a>
				<br>
				<br>
				I was a teaching assistant for the <a href="https://www.polymtl.ca/programmes/cours/iatech-probabilistes-et-dapprentissage" target="_blank">INF8225: Probabilistic Learning</a> course at Polytechnique University taught by <a href="https://mila.quebec/en/person/pal-christopher/" target="_blank">Christopher J. Pal</a> for the Winter 2022 semester.

			</td>
			</tr>

			<tr>
				<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/4f/%C3%89cole_Polytechnique_de_Montr%C3%A9al_Logo.svg/1200px-%C3%89cole_Polytechnique_de_Montr%C3%A9al_Logo.svg.png" height="140" width="150"></td>
				<td width="75%" valign="center">
					<a href="https://chandar-lab.github.io/INF8245E/" target="_blank">
						<papertitle>INF8245E: Machine Learning</papertitle>
					</a>
					<br>
					<br>
					I was a teaching assistant for the <a href="https://chandar-lab.github.io/INF8245E/" target="_blank">INF8245E: Machine Learning</a> course at Polytechnique University taught by <a href="http://sarathchandar.in/" target="_blank">Sarath Chandar</a> for the Fall 2023 semester.
	
				</td>
				</tr>

				<tr>
					<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/udem.jpeg" height="110" width="110"></td>
					<td width="75%" valign="center">
						<a href="http://mitliagkas.github.io/ift6390-ml-class/" target="_blank">
							<papertitle>IFT6390: Machine Learning</papertitle>
						</a>
						<br>
						<br>
						I was a teaching assistant for the <a href="http://mitliagkas.github.io/ift6390-ml-class/" target="_blank">IFT6390: Machine Learning</a> course at UdeM taught by <a href="http://mitliagkas.github.io/" target="_blank">Ioannis Mitliagkis</a> for the Fall 2023 semester.
		
					</td>
					</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/udem.jpeg" height="110" width="110"></td>
						<td width="75%" valign="center">
							<a href="https://sites.google.com/view/winter2024-towards-agi-course" target="_blank">
								<papertitle>IFT6760A: Towards AGI: Scaling, Emergence, Alignment</papertitle>
							</a>
							<br>
							<br>
							I am a teaching assistant for the <a href="https://sites.google.com/view/winter2024-towards-agi-course" target="_blank">IFT6760A: Towards AGI: Scaling, Emergence, Alignment</a> course at UdeM taught by <a href="https://sites.google.com/view/irinarish/home?authuser=0" target="_blank">Irina Rish</a> for the Winter 2024 semester.
						</td>
						</tr>

			  <tr>
					<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
					<td width="75%" valign="center">
						<a href="https://mila-dl-theory.github.io/" target="_blank">
							<papertitle>Deep Learning Theory Reading Group, Mila</papertitle>
						</a>
						<br>
						<br>
						I was an organizer of the DL Theory Reading Group at Mila, Montreal.
					</td>
					</tr>

				<tr>
					<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
					<td width="75%" valign="center">
						<papertitle>Mila 2022 Entrepreneurs Cohort Program</papertitle>
						<br>
						<br>
						I was selected as one of the entrepreneurs in residence and pitched my startup idea called <textbf>9<span style="color:rgb(255, 221, 0)">CLEF</span></textbf> (<a href="https://pitch.com/public/b150fe1f-47f0-439f-9ee4-58aad07b8d57" target="_blank">Elevator Pitch</a>).
					</td>
					</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
						<td width="75%" valign="center">
							<papertitle>Mila 2022 TRAIL</papertitle>
							<br>
							<br>
							I was selected as one of the first students in the Trustworthy Responsible AI Learning certificate (TRAIL) program at Mila, Montreal. (<a href="https://www.virtualbadge.io/certificate-validator?credential=182851c7-c05e-4ab1-a05c-4b276f75da84" target="_blank">Certificate</a>)
						</td>
						</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://i0.wp.com/mcmedhacks.com/wp-content/uploads/2021/05/coloredmedhackslogo.png?fit=1890%2C1890&ssl=1" height="150" width="150"></td>
						<td width="75%" valign="center">
							<papertitle>McMedHacks 2023</papertitle>
							<br>
							<br>
							I was selected as one of the mentors of <a href="https://mcmedhacks.com/" target="_blank">McMedHacks 2023</a> and will be mentoring the participants on the topic of AI in Healthcare.
						</td>
					</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/en/0/08/Logo_for_Conference_on_Neural_Information_Processing_Systems.svg" height="150" width="300"></td>
						<td width="75%" valign="center">
							<papertitle>NewinML workshop @ NeurIPS, 2023</papertitle>
							<br>
							<br>
							I am serving as an organizer, Program Chair (PC) and Area Chair (AC) for the <a href="https://nehzux.github.io/NewInML2022NeurIPS/" target="_blank">NewinML workshop</a> at NeurIPS 2023.
						</td>
					</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/en/0/08/Logo_for_Conference_on_Neural_Information_Processing_Systems.svg" height="150" width="300"></td>
						<td width="75%" valign="center">
							<papertitle>Volunteer @ NeurIPS, 2023</papertitle>
							<br>
							<br>
							I will be serving as a volunteer at the NeurIPS, 2023 conference.
						</td>
					</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://jmlr.org/tmlr/img/tmlr.jpg" height="120" width="180"></td>
						<td width="75%" valign="center">
								<papertitle>Served as a Reviewer (R)/ Program Committee (PC) member for:</papertitle>
							<br>
							<br>
							<a href="https://aistats.org/aistats2025/" target="_blank">AISTATS 2025 (R)</a>, <a href="https://iclr.cc/" target="_blank">ICLR 2025 (R)</a>, <a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank">AAAI 2025 (PC)</a>, <a href="https://eccv2024.ecva.net/" target="_blank">ECCV 2024 (R)</a>,<a href="https://cvpr.thecvf.com/" target="blank">CVPR 2024 (R)</a>, <a href="https://dmlr.ai/" target="_blank">Workshop on Data-centric Machine Learning Research (DMLR): Harnessing Momentum for Science @ ICLR, 2024 (R)</a>, <a href="https://set-llm.github.io/" target="_blank">Workshop on Secure and Trustworthy Large Language Models @ ICLR, 2024 (R)</a>, <a href="https://jmlr.org/tmlr/" target="_blank">TMLR (R)</a>, <a href="https://lifelong-ml.cc/" target="_blank">Conference on Lifelong Learning Agents(CoLLA) 2022 (PC)</a>, <a href="https://lifelong-ml.cc/" target="_blank">Conference on Lifelong Learning Agents(CoLLA) 2023, 2024 (R)</a>, <a href="https://2023.ieeeicassp.org/" target="_blank">ICASSP 2023 (R)</a> (<a href="https://drive.google.com/file/d/1N270w8YLtIN6bSu48dqCSnm9gy2GSlNZ/view?usp=sharing" target="_blank">Certificate</a>), <a href="https://es-fomo.com/" target="_blank">Efficient Systems for Foundation Models workshop at ICLR 2023 (R)</a>, <a href="https://unconf.continualai.org/" target="_blank">Continual Learning AI Un-Conference (R)</a>, <a href="https://www.springer.com/journal/500" target="_blank">Springer Soft Computing (R)</a>.
						  
						</td>
					  </tr>
			
			
					<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
					  <tr>
						<td>
							<a id="achievements"><heading>Achievements</heading></a>
						  <br />(Complete list available upon request)
						</td>
					  </tr>
					</tbody></table>
					<table width="100%" align="center" border="0" cellpadding="20"><tbody>
						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://icm.is.mpg.de/uploads/ckeditor/pictures/1403/content_MPG_IS_Logo_RGB_green-ENG600_ppi.png" alt="nthu" width="300" height="80"></td>
							<td width="75%" valign="center">
									<papertitle>IMPRS-IS Doctoral Fellowship <br /> 2025-2028</papertitle>
								<br>
								<br>
								I was awarded the prestigious <a href="https://imprs.is.mpg.de/" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS) doctoral fellowship</a> for the duration of my PhD under the supervision of <a href="https://orvi.altervista.org/" target="_blank">Prof. Antonio Orvieto</a>.
							</td>
						  </tr>
						
						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://ellis.eu/uploads/ckeditor/pictures/9/content_ellis_logo1_320_transparent.png" height="130" width="130"></td>
							<td width="75%" valign="center">
									<papertitle>ELLIS PhD Fellowship <br /> 2024-2027</papertitle>
								<br>
								<br>
								I was awarded the prestigious <a href="https://ellis.eu/" target="_blank">ELLIS PhD Fellowship</a> as part of which I will be spending my PhD partially at the <a href="https://www.epfl.ch/labs/lions/" target="_blank">Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland</a> supervised by <a href="https://people.epfl.ch/volkan.cevher?lang=en" target="_blank">Prof. Volkan Cevher</a>.
							</td>
						  </tr>
						  
						  <tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/amazon.png" height="100" width="350"></td>
							<td width="75%" valign="center">
									<papertitle>Amazon Science Hub PhD Fellowship <br /> 2024-2027</papertitle>
								<br>
								<br>
								I was awarded the Amazon Science Hub PhD Fellowship for the duration of my PhD from 2024-2027.
							  
							</td>
						  </tr>


					  <tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/fr/thumb/6/6e/%C3%89ducation_et_Enseignement_sup%C3%A9rieur_Qu%C3%A9bec.svg/2560px-%C3%89ducation_et_Enseignement_sup%C3%A9rieur_Qu%C3%A9bec.svg.png" height="100" width="240"></td>
						<td width="75%" valign="center">
								<papertitle>Quebec Ministry of Higher Education International Students Scholarship <br /> 2022</papertitle>
							<br>
							<br>
							I was awarded the DIRO x Quebec Ministry of Higher Education international students scholarship worth CAD$4,000 for the academic year 2022.
						  
						</td>
					  </tr>

					  <tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/fr/thumb/6/6e/%C3%89ducation_et_Enseignement_sup%C3%A9rieur_Qu%C3%A9bec.svg/2560px-%C3%89ducation_et_Enseignement_sup%C3%A9rieur_Qu%C3%A9bec.svg.png" height="100" width="240"></td>
						<td width="75%" valign="center">
								<papertitle>Quebec Ministry of Higher Education International Students Scholarship <br /> 2023</papertitle>
							<br>
							<br>
							I was awarded the DIRO x Quebec Ministry of Higher Education international students scholarship worth CAD$3,000 for the academic year 2023.
						  
						</td>
					  </tr>
					  <tr>
						  <td style="padding:20px;width:25%" valign="center" align="center"><img src="https://pbs.twimg.com/profile_images/1195380768819892229/hkmnqsWf_400x400.jpg" height="120" width="120"></td>
						  <td width="75%" valign="center">
							  <a href="https://www.unique.quebec/2022-unique-excellence-scholarships" target="_blank">
								  <papertitle>UNIQUE AI Excellence Scholarship <br /> 2022</papertitle>
							  </a>
							  <br>
							  <br>
							  I was awarded the UNIQUE AI Excellence Scholarship worth CAD$10,000 for the academic year 2022. Under this scholarship, I will be working with <a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a> and <a href="https://www.mcgill.ca/physiology/directory/core-faculty/pouya-bashivan" target="_blank">Pouya Bashivan</a> on dynamic sparsity based research.
							
						  </td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://pbs.twimg.com/media/ELsiiN0WsAAkUYN.png" height="140" width="300"></td>
							<td width="75%" valign="center">
								<a href="https://paperswithcode.com/" target="_blank">
									<papertitle>PaperswithCode Top Contributor award <br /> 2022</papertitle>
								</a>
								<br>
								<br>
								I was awarded the PaperswithCode Top Contributor award for the academic year 2022.
							  
							</td>
						  </tr>

						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
							<td width="75%" valign="center">
								<papertitle>MILA Entrepreneurs Grant<br /> 2022</papertitle>
								<br>
								<br>
								I was awarded the MILA Entrepreneurs Grant worth CAD$5,000 to pursue my startup venture <textbf>9<span style="color:rgb(255, 221, 0)">CLEF</span></textbf> (<a href="https://pitch.com/public/b150fe1f-47f0-439f-9ee4-58aad07b8d57" target="_blank">Elevator Pitch</a>) and build an early prototype.
							  
							</td>
						  </tr>

						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://images.squarespace-cdn.com/content/v1/619d40013a91687ef63e62d7/7e0aa3da-6cc6-4413-b279-2e9e4a88cab1/Amii-AI-Week-logo.png?format=1500w" height="60" width="240"></td>
							<td width="75%" valign="center">
								<a href="https://www.ai-week.ca/?utm_source=google-ads&utm_medium=cpc&utm_campaign=ai-week&utm_term=amii%20ai%20week&utm_campaign=AI-Week+%7C+S+%7C+Brand&utm_source=adwords&utm_medium=ppc&hsa_acc=6591753441&hsa_cam=16953749208&hsa_grp=135907011819&hsa_ad=593686735388&hsa_src=g&hsa_tgt=kwd-1650174358069&hsa_kw=amii%20ai%20week&hsa_mt=p&hsa_net=adwords&hsa_ver=3&gclid=CjwKCAjwve2TBhByEiwAaktM1BjAxiVdVUehV3fuuvfAgtH1vgzVT_jb-fmmTT6sbtfQSoxJ1RTJihoCLykQAvD_BwE" target="_blank">
									<papertitle>AMII AI Week Travel Bursary<br /> 2022</papertitle>
								</a>
								<br>
								<br>
								I was awarded the AMII AI Week 2022 Student Travel Bursary worth CAD$1,500.
							</td>
						  </tr>
			
						
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                  Updated on: 6th June, 2025
    
              <span style="float:right;">
                Danke Sch√∂n, <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>!
              </span>
                
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
</body>

</html>
